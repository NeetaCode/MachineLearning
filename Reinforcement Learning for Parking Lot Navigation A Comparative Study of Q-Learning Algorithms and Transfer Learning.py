# -*- coding: utf-8 -*-
"""RL_HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kV1dTDP83Wbi7HwQcRoKAhYCE4_1UAHU

Q Learning Algorithm
"""

import matplotlib.pyplot as plt
import numpy as np

# Define parking lot grid
GRID_HEIGHT = 7
GRID_WIDTH = 6
ENTRY_POINT = (0, 5)
AVAILABLE_PARKING_SPOTS = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 1), (4, 2), (4, 3), (4, 4)]
BARRIER_SPOTS = [(3, 1), (3, 2), (3, 3), (3, 4)]

# Define actions (move up, down, left, right)
ACTIONS = [(1, 0), (-1, 0), (0, -1), (0, 1)]

# Create a function to visualize the grid, parking spots, and barriers
def visualize_grid():
    # Create a figure and axis
    fig, ax = plt.subplots()

    # Plot the grid
    ax.set_xticks(range(GRID_WIDTH))
    ax.set_yticks(range(GRID_HEIGHT))
    ax.grid(True, linestyle='-', color='k', linewidth=1)

    # Plot available parking spots
    for spot in AVAILABLE_PARKING_SPOTS:
        ax.add_patch(plt.Rectangle((spot[1], spot[0]), 1, 1, color='green'))

    # Plot barrier spots
    for spot in BARRIER_SPOTS:
        ax.add_patch(plt.Rectangle((spot[1], spot[0]), 1, 1, color='red'))

    # Plot entry point
    ax.add_patch(plt.Rectangle((ENTRY_POINT[1], ENTRY_POINT[0]), 1, 1, color='blue'))

    # Set axis limits and aspect
    ax.set_xlim(0, GRID_WIDTH)
    ax.set_ylim(0, GRID_HEIGHT)
    ax.set_aspect('equal')

    # Show plot
    plt.title("Parking Lot Grid")
    plt.xlabel("Columns")
    plt.ylabel("Rows")
    plt.show()

# Visualize the grid
visualize_grid()

"""Q Learning"""

import numpy as np
import matplotlib.pyplot as plt

# Initialize Q-values and final parked spots
Q = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
final_parked_spots = []

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.1  # exploration rate

# Define rewards
REWARD_GOAL = 100
REWARD_BARRIER = -75
REWARD_CORRECT_ACTION = 5

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation
def update_Q_value(state, action, reward, next_state):
    max_next_Q = np.max(Q[next_state])
    current_Q = Q[state][ACTIONS.index(action)]
    Q[state][ACTIONS.index(action)] = current_Q + alpha * (reward + gamma * max_next_Q - current_Q)

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Q-learning algorithm
def q_learning():
    global final_parked_spots
    episodes = 1000
    rewards = []
    for _ in range(episodes):
        current_state = ENTRY_POINT
        total_reward = 0

        while current_state not in AVAILABLE_PARKING_SPOTS:
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                action = ACTIONS[np.argmax(Q[current_state])]

            next_states = get_possible_next_states(current_state, action)

            if next_states:
                next_state = next_states[np.random.choice(range(len(next_states)))]

                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = REWARD_GOAL
                elif next_state in BARRIER_SPOTS:
                    reward = REWARD_BARRIER
                else:
                    reward = REWARD_CORRECT_ACTION

                total_reward += reward

                update_Q_value(current_state, action, reward, next_state)

                current_state = next_state
            else:
                # If there are no possible next states, stay at the current state
                current_state = current_state

        final_parked_spots.append(current_state)  # Record the final parked spot
        rewards.append(total_reward)

    return rewards

# Function to visualize the learned policy
def visualize_policy():
    policy = np.argmax(Q, axis=2)
    fig, ax = plt.subplots()
    ax.set_xticks(range(GRID_WIDTH))
    ax.set_yticks(range(GRID_HEIGHT))
    ax.grid(True, linestyle='-', color='k', linewidth=1)
    ax.set_xlim(0, GRID_WIDTH)
    ax.set_ylim(0, GRID_HEIGHT)
    ax.set_aspect('equal')

    for i in range(GRID_HEIGHT):
        for j in range(GRID_WIDTH):
            if (i, j) == ENTRY_POINT:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='blue'))  # Entry point
                ax.text(j + 0.5, i + 0.5, 'START', ha='center', va='center', fontsize=8)
            elif (i, j) in AVAILABLE_PARKING_SPOTS:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='green'))  # Available parking spots
                ax.text(j + 0.5, i + 0.5, 'PARK', ha='center', va='center', fontsize=8)
            elif (i, j) in BARRIER_SPOTS:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='red'))  # Barriers
                ax.text(j + 0.5, i + 0.5, 'BARRIER', ha='center', va='center', fontsize=8)
            else:
                arrow_direction = ACTIONS[policy[i, j]]
                ax.arrow(j + 0.5, i + 0.5, arrow_direction[1]*0.3, arrow_direction[0]*0.3,
                         head_width=0.1, head_length=0.1, fc='k', ec='k')  # Arrows representing policy
                action_label = ['UP', 'DOWN', 'LEFT', 'RIGHT'][np.argmax(Q[i, j])]
                ax.text(j + 0.5, i + 0.5, action_label, ha='center', va='center', fontsize=8)

    plt.title("Learned Policy and Q-table")
    plt.xlabel("Columns")
    plt.ylabel("Rows")
    plt.show()

# Run Q-learning algorithm
rewards = q_learning()

# # Display Q-values
# print("Q-values:")
# print(Q)

# Plot rewards per episode
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode')
plt.show()

# Visualize the learned policy
visualize_policy()

# Display final parked spots
print("Final parked spots by the car:")
print(final_parked_spots)

"""Double Q Learning"""

import numpy as np
import matplotlib.pyplot as plt

# Initialize Q-values and final parked spots for both Q1 and Q2
Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
final_parked_spots = []

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.1  # exploration rate

# Define rewards
REWARD_GOAL = 100
REWARD_BARRIER = -75
REWARD_CORRECT_ACTION = 5

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation
def update_Q_value(Q, state, action, reward, next_state):
    max_next_Q = np.max(Q[next_state])
    current_Q = Q[state][ACTIONS.index(action)]
    Q[state][ACTIONS.index(action)] = current_Q + alpha * (reward + gamma * max_next_Q - current_Q)

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Q-learning algorithm
def double_q_learning():
    global final_parked_spots
    episodes = 1000
    rewards = []
    for _ in range(episodes):
        current_state = ENTRY_POINT
        total_reward = 0

        while current_state not in AVAILABLE_PARKING_SPOTS:
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                # Randomly select Q1 or Q2 for action selection
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            next_states = get_possible_next_states(current_state, action)

            if next_states:
                next_state = next_states[np.random.choice(range(len(next_states)))]

                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = REWARD_GOAL
                elif next_state in BARRIER_SPOTS:
                    reward = REWARD_BARRIER
                else:
                    reward = REWARD_CORRECT_ACTION

                total_reward += reward

                # Update Q-values using the other Q-table
                if np.random.uniform(0, 1) < 0.5:
                    update_Q_value(Q1, current_state, action, reward, next_state)
                else:
                    update_Q_value(Q2, current_state, action, reward, next_state)

                current_state = next_state
            else:
                # If there are no possible next states, stay at the current state
                current_state = current_state

        final_parked_spots.append(current_state)  # Record the final parked spot
        rewards.append(total_reward)

    return rewards

# Function to visualize the learned policy
def visualize_policy(Q):
    policy = np.argmax(Q, axis=2)
    fig, ax = plt.subplots()
    ax.set_xticks(range(GRID_WIDTH))
    ax.set_yticks(range(GRID_HEIGHT))
    ax.grid(True, linestyle='-', color='k', linewidth=1)
    ax.set_xlim(0, GRID_WIDTH)
    ax.set_ylim(0, GRID_HEIGHT)
    ax.set_aspect('equal')

    for i in range(GRID_HEIGHT):
        for j in range(GRID_WIDTH):
            if (i, j) == ENTRY_POINT:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='blue'))  # Entry point
                ax.text(j + 0.5, i + 0.5, 'START', ha='center', va='center', fontsize=8)
            elif (i, j) in AVAILABLE_PARKING_SPOTS:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='green'))  # Available parking spots
                ax.text(j + 0.5, i + 0.5, 'PARK', ha='center', va='center', fontsize=8)
            elif (i, j) in BARRIER_SPOTS:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='red'))  # Barriers
                ax.text(j + 0.5, i + 0.5, 'BARRIER', ha='center', va='center', fontsize=8)
            else:
                arrow_direction = ACTIONS[policy[i, j]]
                ax.arrow(j + 0.5, i + 0.5, arrow_direction[1]*0.3, arrow_direction[0]*0.3,
                         head_width=0.1, head_length=0.1, fc='k', ec='k')  # Arrows representing policy
                action_label = ['UP', 'DOWN', 'LEFT', 'RIGHT'][np.argmax(Q[i, j])]
                ax.text(j + 0.5, i + 0.5, action_label, ha='center', va='center', fontsize=8)

    plt.title("Learned Policy and Q-table")
    plt.xlabel("Columns")
    plt.ylabel("Rows")
    plt.show()

# Run Double Q-learning algorithm
rewards = double_q_learning()

# # Display Q-values
# print("Q1-values:")
# print(Q1)
# print("Q2-values:")
# print(Q2)

# Plot rewards per episode
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode')
plt.show()

# Visualize the learned policy for Q1
print("Learned Policy for Q1:")
visualize_policy(Q1)

# Visualize the learned policy for Q2
print("Learned Policy for Q2:")
visualize_policy(Q2)

# Display final parked spots
print("Final parked spots by the car:")
print(final_parked_spots)

"""Self Correcting Q Learning"""

import numpy as np
import matplotlib.pyplot as plt

# Initialize Q-values and final parked spots for both Q1 and Q2
Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
final_parked_spots = []

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.1  # exploration rate
update_interval = 50  # interval for checking and correcting Q-values
correction_factor = 0.1  # factor to adjust Q-values towards the other Q-table

# Define rewards
REWARD_GOAL = 100
REWARD_BARRIER = -75
REWARD_CORRECT_ACTION = 5

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation
def update_Q_value(Q, state, action, reward, next_state):
    max_next_Q = np.max(Q[next_state])
    current_Q = Q[state][ACTIONS.index(action)]
    Q[state][ACTIONS.index(action)] = current_Q + alpha * (reward + gamma * max_next_Q - current_Q)

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Helper function to correct Q-values towards the other Q-table
def correct_Q_values(Q, other_Q):
    for i in range(GRID_HEIGHT):
        for j in range(GRID_WIDTH):
            for a in range(len(ACTIONS)):
                Q[i, j, a] = Q[i, j, a] + correction_factor * (other_Q[i, j, a] - Q[i, j, a])

# Q-learning algorithm
def self_correcting_q_learning():
    global final_parked_spots
    episodes = 1000
    rewards = []
    for _ in range(episodes):
        current_state = ENTRY_POINT
        total_reward = 0

        while current_state not in AVAILABLE_PARKING_SPOTS:
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                # Randomly select Q1 or Q2 for action selection
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            next_states = get_possible_next_states(current_state, action)

            if next_states:
                next_state = next_states[np.random.choice(range(len(next_states)))]

                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = REWARD_GOAL
                elif next_state in BARRIER_SPOTS:
                    reward = REWARD_BARRIER
                else:
                    reward = REWARD_CORRECT_ACTION

                total_reward += reward

                # Update Q-values using the other Q-table
                if np.random.uniform(0, 1) < 0.5:
                    update_Q_value(Q1, current_state, action, reward, next_state)
                else:
                    update_Q_value(Q2, current_state, action, reward, next_state)

                current_state = next_state

                # Periodically correct Q-values
                if _ % update_interval == 0:
                    correct_Q_values(Q1, Q2)
                    correct_Q_values(Q2, Q1)
            else:
                # If there are no possible next states, stay at the current state
                current_state = current_state

        final_parked_spots.append(current_state)  # Record the final parked spot
        rewards.append(total_reward)

    return rewards

# Function to visualize the learned policy
def visualize_policy(Q):
    policy = np.argmax(Q, axis=2)
    fig, ax = plt.subplots()
    ax.set_xticks(range(GRID_WIDTH))
    ax.set_yticks(range(GRID_HEIGHT))
    ax.grid(True, linestyle='-', color='k', linewidth=1)
    ax.set_xlim(0, GRID_WIDTH)
    ax.set_ylim(0, GRID_HEIGHT)
    ax.set_aspect('equal')

    for i in range(GRID_HEIGHT):
        for j in range(GRID_WIDTH):
            if (i, j) == ENTRY_POINT:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='blue'))  # Entry point
                ax.text(j + 0.5, i + 0.5, 'START', ha='center', va='center', fontsize=8)
            elif (i, j) in AVAILABLE_PARKING_SPOTS:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='green'))  # Available parking spots
                ax.text(j + 0.5, i + 0.5, 'PARK', ha='center', va='center', fontsize=8)
            elif (i, j) in BARRIER_SPOTS:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='red'))  # Barriers
                ax.text(j + 0.5, i + 0.5, 'BARRIER', ha='center', va='center', fontsize=8)
            else:
                arrow_direction = ACTIONS[policy[i, j]]
                ax.arrow(j + 0.5, i + 0.5, arrow_direction[1]*0.3, arrow_direction[0]*0.3,
                         head_width=0.1, head_length=0.1, fc='k', ec='k')  # Arrows representing policy
                action_label = ['UP', 'DOWN', 'LEFT', 'RIGHT'][np.argmax(Q[i, j])]
                ax.text(j + 0.5, i + 0.5, action_label, ha='center', va='center', fontsize=8)

    plt.title("Learned Policy and Q-table")
    plt.xlabel("Columns")
    plt.ylabel("Rows")
    plt.show()

# Run Self-Correcting Q-learning algorithm
rewards = self_correcting_q_learning()

# # Display Q-values
# print("Q1-values:")
# print(Q1)
# print("Q2-values:")
# print(Q2)

# Plot rewards per episode
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode')
plt.show()

# Visualize the learned policy for Q1
print("Learned Policy for Q1:")
visualize_policy(Q1)

# Visualize the learned policy for Q2
print("Learned Policy for Q2:")
visualize_policy(Q2)

# Display final parked spots
print("Final parked spots by the car:")
print(final_parked_spots)

# Initialize Q-values for two sets
Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.1  # exploration rate

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation
def update_Q_value(Q1, Q2, state, action, reward, next_state):
    if np.random.uniform(0, 1) < 0.5:
        max_next_Q = np.max(Q1[next_state])
        Q1[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q2[next_state][np.argmax(Q1[next_state])] - Q1[state][ACTIONS.index(action)])
    else:
        max_next_Q = np.max(Q2[next_state])
        Q2[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q1[next_state][np.argmax(Q2[next_state])] - Q2[state][ACTIONS.index(action)])

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    # Check if the next states are within the grid
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Double Q-learning algorithm
def double_q_learning():
    episodes = 1000
    rewards = []
    for _ in range(episodes):
        # Reset the car's position to the entry point
        current_state = ENTRY_POINT
        total_reward = 0

        # Loop until the car parks in an available spot
        while current_state not in AVAILABLE_PARKING_SPOTS:
            # Choose an action using epsilon-greedy policy
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            # Get possible next states
            next_states = get_possible_next_states(current_state, action)

            if next_states:
                # Choose next state
                next_state = next_states[np.random.choice(range(len(next_states)))]

                # Calculate reward
                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = 1
                elif next_state in BARRIER_SPOTS or not within_grid(next_state):
                    reward = -1
                else:
                    reward = 0

                total_reward += reward

                # Update Q-value
                update_Q_value(Q1, Q2, current_state, action, reward, next_state)

                # Move to the next state
                current_state = next_state
            else:
                # If there are no possible next states, reset the current state to the entry point
                current_state = ENTRY_POINT

        rewards.append(total_reward)

    return rewards

# Run Double Q-learning algorithm
rewards = double_q_learning()

# Display Q-values
print("Q1-values:")
print(Q1)
print("\nQ2-values:")
print(Q2)

# Plot rewards per episode
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode (Double Q-learning)')
plt.show()

# Initialize Q-values for two sets
Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.2  # exploration rate (increased)

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation
def update_Q_value(Q1, Q2, state, action, reward, next_state):
    if np.random.uniform(0, 1) < 0.5:
        max_next_Q = np.max(Q1[next_state])
        Q1[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q2[next_state].max() - Q1[state][ACTIONS.index(action)])
    else:
        max_next_Q = np.max(Q2[next_state])
        Q2[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q1[next_state].max() - Q2[state][ACTIONS.index(action)])

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    # Check if the next states are within the grid
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Double Q-learning algorithm with modified exploration rate
def double_q_learning():
    episodes = 1000
    rewards = []
    for _ in range(episodes):
        # Reset the car's position to the entry point
        current_state = ENTRY_POINT
        total_reward = 0

        # Loop until the car parks in an available spot
        while current_state not in AVAILABLE_PARKING_SPOTS:
            # Choose an action using epsilon-greedy policy with increased exploration rate
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            # Get possible next states
            next_states = get_possible_next_states(current_state, action)

            if next_states:
                # Choose next state
                next_state = next_states[np.random.choice(range(len(next_states)))]

                # Calculate reward
                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = 1
                elif next_state in BARRIER_SPOTS or not within_grid(next_state):
                    reward = -1
                else:
                    reward = 0

                total_reward += reward

                # Update Q-value
                update_Q_value(Q1, Q2, current_state, action, reward, next_state)

                # Move to the next state
                current_state = next_state
            else:
                # If there are no possible next states, reset the current state to the entry point
                current_state = ENTRY_POINT

        rewards.append(total_reward)

    return rewards

# Run Double Q-learning algorithm with modified parameters
rewards = double_q_learning()

# Display Q-values
print("Q1-values:")
print(Q1)
print("\nQ2-values:")
print(Q2)

# Plot rewards per episode
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode (Double Q-learning)')
plt.show()

# Initialize Q-values for two sets
Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))

# Define parameters
alphas = [0.1, 0.2, 0.5]  # learning rates to test
gammas = [0.5, 0.9, 0.99]  # discount factors to test
epsilons = [0.1, 0.2, 0.5]  # exploration rates to test
episodes = 1000

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation with self-correcting mechanism
def update_Q_value(Q1, Q2, state, action, reward, next_state, alpha, gamma):
    if np.random.uniform(0, 1) < 0.5:
        max_next_Q = np.max(Q1[next_state])
        Q1[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q2[next_state][np.argmax(Q1[next_state])] - Q1[state][ACTIONS.index(action)])
    else:
        max_next_Q = np.max(Q2[next_state])
        Q2[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q1[next_state][np.argmax(Q2[next_state])] - Q2[state][ACTIONS.index(action)])

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    # Check if the next states are within the grid
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Double Q-learning algorithm with self-correcting mechanism
def double_q_learning(alpha, gamma, epsilon):
    rewards = []
    for _ in range(episodes):
        # Reset the car's position to the entry point
        current_state = ENTRY_POINT
        total_reward = 0

        # Loop until the car parks in an available spot
        while current_state not in AVAILABLE_PARKING_SPOTS:
            # Choose an action using epsilon-greedy policy with increased exploration rate
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            # Get possible next states
            next_states = get_possible_next_states(current_state, action)

            if next_states:
                # Choose next state
                next_state = next_states[np.random.choice(range(len(next_states)))]

                # Calculate reward
                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = 1
                elif next_state in BARRIER_SPOTS or not within_grid(next_state):
                    reward = -1
                else:
                    reward = 0

                total_reward += reward

                # Update Q-value with self-correcting mechanism
                update_Q_value(Q1, Q2, current_state, action, reward, next_state, alpha, gamma)

                # Move to the next state
                current_state = next_state
            else:
                # If there are no possible next states, reset the current state to the entry point
                current_state = ENTRY_POINT

        rewards.append(total_reward)

    return rewards

# Function to run experiments with different parameter values
def run_experiments():
    results = {}
    for alpha in alphas:
        for gamma in gammas:
            for epsilon in epsilons:
                key = (alpha, gamma, epsilon)
                rewards = double_q_learning(alpha, gamma, epsilon)
                results[key] = rewards
    return results

# Run experiments
results = run_experiments()

# Plot rewards per episode for different parameter values
plt.figure(figsize=(12, 8))
for key, rewards in results.items():
    alpha, gamma, epsilon = key
    label = f'alpha={alpha}, gamma={gamma}, epsilon={epsilon}'
    plt.plot(rewards, label=label)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode (Double Q-learning with Self-Correcting)')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Define parking lot grid for the first task
GRID_HEIGHT_1 = 7
GRID_WIDTH_1 = 6
ENTRY_POINT_1 = (0, 5)
AVAILABLE_PARKING_SPOTS_1 = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 1), (4, 2), (4, 3), (4, 4)]
BARRIER_SPOTS_1 = [(3, 1), (3, 2), (3, 3), (3, 4)]

# Define parking lot grid for the second task
GRID_HEIGHT_2 = 5
GRID_WIDTH_2 = 5
ENTRY_POINT_2 = (0, 4)
AVAILABLE_PARKING_SPOTS_2 = [(2, 1), (2, 2), (2, 3), (2, 4)]
BARRIER_SPOTS_2 = [(1, 1), (1, 2), (1, 3), (1, 4)]

# Define actions (move up, down, left, right)
ACTIONS = [(1, 0), (-1, 0), (0, -1), (0, 1)]

# Initialize Q-values for two sets for the first task
Q1_1 = np.zeros((GRID_HEIGHT_1, GRID_WIDTH_1, len(ACTIONS)))
Q2_1 = np.zeros((GRID_HEIGHT_1, GRID_WIDTH_1, len(ACTIONS)))

# Initialize Q-values for two sets for the second task (seeded with Q-values from the first task)
Q1_2 = np.copy(Q1_1)
Q2_2 = np.copy(Q2_1)

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.2  # exploration rate (increased)
episodes = 1000

# Helper function to check if a position is within the grid
def within_grid(position, height, width):
    return 0 <= position[0] < height and 0 <= position[1] < width

# Helper function to update Q-values based on Q-learning equation with self-correcting mechanism
def update_Q_value(Q1, Q2, state, action, reward, next_state, alpha, gamma):
    if np.random.uniform(0, 1) < 0.5:
        max_next_Q = np.max(Q1[next_state])
        Q1[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q2[next_state][np.argmax(Q1[next_state])] - Q1[state][ACTIONS.index(action)])
    else:
        max_next_Q = np.max(Q2[next_state])
        Q2[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q1[next_state][np.argmax(Q2[next_state])] - Q2[state][ACTIONS.index(action)])

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action, height, width):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    # Check if the next states are within the grid
    next_states = [s for s in next_states if within_grid(s, height, width)]
    return next_states

# Double Q-learning algorithm with self-correcting mechanism
def double_q_learning(Q1, Q2, height, width, entry_point, available_parking_spots, barrier_spots):
    rewards = []
    for _ in range(episodes):
        # Reset the car's position to the entry point
        current_state = entry_point
        total_reward = 0

        # Loop until the car parks in an available spot
        while current_state not in available_parking_spots:
            # Choose an action using epsilon-greedy policy with increased exploration rate
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            # Get possible next states
            next_states = get_possible_next_states(current_state, action, height, width)

            if next_states:
                # Choose next state
                next_state = next_states[np.random.choice(range(len(next_states)))]

                # Calculate reward
                if next_state in available_parking_spots:
                    reward = 1
                elif next_state in barrier_spots or not within_grid(next_state, height, width):
                    reward = -1
                else:
                    reward = 0

                total_reward += reward

                # Update Q-value with self-correcting mechanism
                update_Q_value(Q1, Q2, current_state, action, reward, next_state, alpha, gamma)

                # Move to the next state
                current_state = next_state
            else:
                # If there are no possible next states, reset the current state to the entry point
                current_state = entry_point

        rewards.append(total_reward)

    return rewards

# Perform transfer learning from the first task to the second task
rewards_1 = double_q_learning(Q1_1, Q2_1, GRID_HEIGHT_1, GRID_WIDTH_1, ENTRY_POINT_1, AVAILABLE_PARKING_SPOTS_1, BARRIER_SPOTS_1)
rewards_2 = double_q_learning(Q1_2, Q2_2, GRID_HEIGHT_2, GRID_WIDTH_2, ENTRY_POINT_2, AVAILABLE_PARKING_SPOTS_2, BARRIER_SPOTS_2)

# Plot rewards per episode for both tasks
plt.figure(figsize=(12, 8))
plt.plot(rewards_1, label='Task 1')
plt.plot(rewards_2, label='Task 2 (Transfer Learning)')
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Define parking lot grid
GRID_HEIGHT = 7
GRID_WIDTH = 6
ENTRY_POINT = (0, 5)
AVAILABLE_PARKING_SPOTS = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 1), (4, 2), (4, 3), (4, 4)]
BARRIER_SPOTS = [(3, 1), (3, 2), (3, 3), (3, 4)]

# Define actions (move up, down, left, right)
ACTIONS = [(1, 0), (-1, 0), (0, -1), (0, 1)]

# Initialize Q-values for two sets
Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.1  # exploration rate

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation with self-correcting mechanism
def update_Q_value(Q1, Q2, state, action, reward, next_state):
    if np.random.uniform(0, 1) < 0.5:
        max_next_Q = np.max(Q1[next_state])
        Q1[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q2[next_state][np.argmax(Q1[next_state])] - Q1[state][ACTIONS.index(action)])
    else:
        max_next_Q = np.max(Q2[next_state])
        Q2[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q1[next_state][np.argmax(Q2[next_state])] - Q2[state][ACTIONS.index(action)])

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    # Check if the next states are within the grid
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Double Q-learning algorithm with self-correcting mechanism
def double_q_learning():
    episodes = 1000
    rewards = []
    for _ in range(episodes):
        # Reset the car's position to the entry point
        current_state = ENTRY_POINT
        total_reward = 0

        # Loop until the car parks in an available spot
        while current_state not in AVAILABLE_PARKING_SPOTS:
            # Choose an action using epsilon-greedy policy
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            # Get possible next states
            next_states = get_possible_next_states(current_state, action)

            if next_states:
                # Choose next state
                next_state = next_states[np.random.choice(range(len(next_states)))]

                # Calculate reward
                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = 1
                elif next_state in BARRIER_SPOTS or not within_grid(next_state):
                    reward = -1
                else:
                    reward = 0

                total_reward += reward

                # Update Q-value with self-correcting mechanism
                update_Q_value(Q1, Q2, current_state, action, reward, next_state)

                # Move to the next state
                current_state = next_state
            else:
                # If there are no possible next states, reset the current state to the entry point
                current_state = ENTRY_POINT

        rewards.append(total_reward)

    return rewards

# Run Double Q-learning algorithm with self-correcting mechanism
rewards = double_q_learning()

# Display Q-values
print("Q1-values:")
print(Q1)
print("\nQ2-values:")
print(Q2)

# Plot rewards per episode
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode (Double Q-learning with Self-Correcting)')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Define parking lot grid
GRID_HEIGHT = 7
GRID_WIDTH = 6
ENTRY_POINT = (0, 5)
AVAILABLE_PARKING_SPOTS = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 1), (4, 2), (4, 3), (4, 4)]
BARRIER_SPOTS = [(3, 1), (3, 2), (3, 3), (3, 4)]

# Define actions (move up, down, left, right)
ACTIONS = [(1, 0), (-1, 0), (0, -1), (0, 1)]

# Initialize Q-values for two sets
Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.2  # exploration rate (increased)

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation with self-correcting mechanism
def update_Q_value(Q1, Q2, state, action, reward, next_state):
    if np.random.uniform(0, 1) < 0.5:
        max_next_Q = np.max(Q1[next_state])
        Q1[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q2[next_state][np.argmax(Q1[next_state])] - Q1[state][ACTIONS.index(action)])
    else:
        max_next_Q = np.max(Q2[next_state])
        Q2[state][ACTIONS.index(action)] += alpha * (reward + gamma * Q1[next_state][np.argmax(Q2[next_state])] - Q2[state][ACTIONS.index(action)])

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    # Check if the next states are within the grid
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Double Q-learning algorithm with self-correcting mechanism
def double_q_learning():
    episodes = 1000
    rewards = []
    for _ in range(episodes):
        # Reset the car's position to the entry point
        current_state = ENTRY_POINT
        total_reward = 0

        # Loop until the car parks in an available spot
        while current_state not in AVAILABLE_PARKING_SPOTS:
            # Choose an action using epsilon-greedy policy with increased exploration rate
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            # Get possible next states
            next_states = get_possible_next_states(current_state, action)

            if next_states:
                # Choose next state
                next_state = next_states[np.random.choice(range(len(next_states)))]

                # Calculate reward
                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = 1
                elif next_state in BARRIER_SPOTS or not within_grid(next_state):
                    reward = -1
                else:
                    reward = 0

                total_reward += reward

                # Update Q-value with self-correcting mechanism
                update_Q_value(Q1, Q2, current_state, action, reward, next_state)

                # Move to the next state
                current_state = next_state
            else:
                # If there are no possible next states, reset the current state to the entry point
                current_state = ENTRY_POINT

        rewards.append(total_reward)

    return rewards

# Run Double Q-learning algorithm with self-correcting mechanism
rewards = double_q_learning()

# Display Q-values
print("Q1-values:")
print(Q1)
print("\nQ2-values:")
print(Q2)

# Plot rewards per episode
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode (Double Q-learning with Self-Correcting)')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Initialize Q-values and final parked spots for both Q1 and Q2
Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
final_parked_spots = []

# Define parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.1  # exploration rate
update_interval = 50  # interval for checking and correcting Q-values
correction_factor = 0.1  # factor to adjust Q-values towards the other Q-table

# Define rewards
REWARD_GOAL = 100
REWARD_BARRIER = -75
REWARD_CORRECT_ACTION = 5

# Helper function to check if a position is within the grid
def within_grid(position):
    return 0 <= position[0] < GRID_HEIGHT and 0 <= position[1] < GRID_WIDTH

# Helper function to update Q-values based on Q-learning equation
def update_Q_value(Q, state, action, reward, next_state):
    max_next_Q = np.max(Q[next_state])
    current_Q = Q[state][ACTIONS.index(action)]
    Q[state][ACTIONS.index(action)] = current_Q + alpha * (reward + gamma * max_next_Q - current_Q)

# Helper function to get possible next states given current state and action
def get_possible_next_states(state, action):
    next_states = [(state[0] + action[0], state[1] + action[1])]
    next_states = [s for s in next_states if within_grid(s)]
    return next_states

# Helper function to correct Q-values towards the other Q-table
def correct_Q_values(Q, other_Q):
    for i in range(GRID_HEIGHT):
        for j in range(GRID_WIDTH):
            for a in range(len(ACTIONS)):
                Q[i, j, a] = Q[i, j, a] + correction_factor * (other_Q[i, j, a] - Q[i, j, a])

# Q-learning algorithm
def self_correcting_q_learning():
    global final_parked_spots
    episodes = 1000
    rewards = []
    for _ in range(episodes):
        current_state = ENTRY_POINT
        total_reward = 0

        while current_state not in AVAILABLE_PARKING_SPOTS:
            if np.random.uniform(0, 1) < epsilon:
                action = ACTIONS[np.random.choice(range(len(ACTIONS)))]
            else:
                # Randomly select Q1 or Q2 for action selection
                if np.random.uniform(0, 1) < 0.5:
                    action = ACTIONS[np.argmax(Q1[current_state])]
                else:
                    action = ACTIONS[np.argmax(Q2[current_state])]

            next_states = get_possible_next_states(current_state, action)

            if next_states:
                next_state = next_states[np.random.choice(range(len(next_states)))]

                if next_state in AVAILABLE_PARKING_SPOTS:
                    reward = REWARD_GOAL
                elif next_state in BARRIER_SPOTS:
                    reward = REWARD_BARRIER
                else:
                    reward = REWARD_CORRECT_ACTION

                total_reward += reward

                # Update Q-values using the other Q-table
                if np.random.uniform(0, 1) < 0.5:
                    update_Q_value(Q1, current_state, action, reward, next_state)
                else:
                    update_Q_value(Q2, current_state, action, reward, next_state)

                current_state = next_state

                # Periodically correct Q-values
                if _ % update_interval == 0:
                    correct_Q_values(Q1, Q2)
                    correct_Q_values(Q2, Q1)
            else:
                # If there are no possible next states, stay at the current state
                current_state = current_state

        final_parked_spots.append(current_state)  # Record the final parked spot
        rewards.append(total_reward)

    return rewards

# Function to visualize the learned policy
def visualize_policy(Q):
    policy = np.argmax(Q, axis=2)
    fig, ax = plt.subplots()
    ax.set_xticks(range(GRID_WIDTH))
    ax.set_yticks(range(GRID_HEIGHT))
    ax.grid(True, linestyle='-', color='k', linewidth=1)
    ax.set_xlim(0, GRID_WIDTH)
    ax.set_ylim(0, GRID_HEIGHT)
    ax.set_aspect('equal')

    for i in range(GRID_HEIGHT):
        for j in range(GRID_WIDTH):
            if (i, j) == ENTRY_POINT:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='blue'))  # Entry point
                ax.text(j + 0.5, i + 0.5, 'START', ha='center', va='center', fontsize=8)
            elif (i, j) in AVAILABLE_PARKING_SPOTS:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='green'))  # Available parking spots
                ax.text(j + 0.5, i + 0.5, 'PARK', ha='center', va='center', fontsize=8)
            elif (i, j) in BARRIER_SPOTS:
                ax.add_patch(plt.Rectangle((j, i), 1, 1, color='red'))  # Barriers
                ax.text(j + 0.5, i + 0.5, 'BARRIER', ha='center', va='center', fontsize=8)
            else:
                arrow_direction = ACTIONS[policy[i, j]]
                ax.arrow(j + 0.5, i + 0.5, arrow_direction[1]*0.3, arrow_direction[0]*0.3,
                         head_width=0.1, head_length=0.1, fc='k', ec='k')  # Arrows representing policy
                action_label = ['UP', 'DOWN', 'LEFT', 'RIGHT'][np.argmax(Q[i, j])]
                ax.text(j + 0.5, i + 0.5, action_label, ha='center', va='center', fontsize=8)

    plt.title("Learned Policy and Q-table")
    plt.xlabel("Columns")
    plt.ylabel("Rows")
    plt.show()

# Run Self-Correcting Q-learning algorithm
rewards = self_correcting_q_learning()

# Display Q-values
print("Q1-values:")
print(Q1)
print("Q2-values:")
print(Q2)

# Plot rewards per episode
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Total Reward per Episode')
plt.show()

# Visualize the learned policy for Q1
print("Learned Policy for Q1:")
visualize_policy(Q1)

# Visualize the learned policy for Q2
print("Learned Policy for Q2:")
visualize_policy(Q2)

# Display final parked spots
print("Final parked spots by the car:")
print(final_parked_spots)

import numpy as np
import random
import matplotlib.pyplot as plt

# Define parking lot grid
GRID_HEIGHT = 6
GRID_WIDTH = 7
ENTRY_POINT = (5, 6)
AVAILABLE_PARKING_SPOTS = [(2, 2), (2, 3), (2, 4), (4, 2), (4, 3), (4, 4)]
BARRIER_SPOTS = [(3, 2), (3, 3), (3, 4), (3, 5)]

# Define actions (move forward, backward, sideways)
ACTIONS = [(1, 0), (-1, 0), (0, 1), (0, -1)]

# Define Q-learning parameters
ALPHA = 0.1  # learning rate
GAMMA = 0.9  # discount factor
EPSILON = 0.1  # exploration rate
NUM_EPISODES = 1000

# Initialize Q-table
Q = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))


def is_valid_move(state, action):
    next_state = (state[0] + action[0], state[1] + action[1])
    if next_state[0] < 0 or next_state[0] >= GRID_HEIGHT or \
            next_state[1] < 0 or next_state[1] >= GRID_WIDTH:
        return False
    elif next_state in AVAILABLE_PARKING_SPOTS or next_state in BARRIER_SPOTS:
        return False
    return True


def get_next_state(state, action):
    next_state = (state[0] + action[0], state[1] + action[1])
    next_state = (max(0, min(next_state[0], GRID_HEIGHT - 1)), max(0, min(next_state[1], GRID_WIDTH - 1)))
    return next_state


def choose_action(state, epsilon):
    if np.random.uniform(0, 1) < epsilon:
        return random.choice(range(len(ACTIONS)))
    else:
        return np.argmax(Q[state[0], state[1]])


def q_learning():
    rewards = []
    for episode in range(NUM_EPISODES):
        state = ENTRY_POINT
        total_reward = 0
        while state not in AVAILABLE_PARKING_SPOTS:
            action = choose_action(state, EPSILON)
            next_state = get_next_state(state, ACTIONS[action])
            reward = -1
            if next_state in AVAILABLE_PARKING_SPOTS:
                reward = 100
            elif next_state in BARRIER_SPOTS or not is_valid_move(state, ACTIONS[action]):
                reward = -10
            Q[state[0], state[1], action] += ALPHA * \
                (reward + GAMMA * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], action])
            total_reward += reward
            state = next_state
        rewards.append(total_reward)
    return rewards


def double_q_learning():
    rewards = []
    Q1 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
    Q2 = np.zeros((GRID_HEIGHT, GRID_WIDTH, len(ACTIONS)))
    for episode in range(NUM_EPISODES):
        state = ENTRY_POINT
        total_reward = 0
        while state not in AVAILABLE_PARKING_SPOTS:
            action = choose_action(state, EPSILON)
            next_state = get_next_state(state, ACTIONS[action])
            reward = -1
            if next_state in AVAILABLE_PARKING_SPOTS:
                reward = 100
            elif next_state in BARRIER_SPOTS or not is_valid_move(state, ACTIONS[action]):
                reward = -10
            if np.random.uniform(0, 1) < 0.5:
                Q1[state[0], state[1], action] += ALPHA * \
                    (reward + GAMMA * Q2[next_state[0], next_state[1], np.argmax(Q1[next_state[0], next_state[1]])] -
                     Q1[state[0], state[1], action])
            else:
                Q2[state[0], state[1], action] += ALPHA * \
                    (reward + GAMMA * Q1[next_state[0], next_state[1], np.argmax(Q2[next_state[0], next_state[1]])] -
                     Q2[state[0], state[1], action])
            total_reward += reward
            state = next_state
        rewards.append(total_reward)
    return rewards


def self_correcting_q_learning():
    rewards = []
    for episode in range(NUM_EPISODES):
        state = ENTRY_POINT
        total_reward = 0
        while state not in AVAILABLE_PARKING_SPOTS:
            action = choose_action(state, EPSILON)
            next_state = get_next_state(state, ACTIONS[action])
            reward = -1
            if next_state in AVAILABLE_PARKING_SPOTS:
                reward = 100
            elif next_state in BARRIER_SPOTS or not is_valid_move(state, ACTIONS[action]):
                reward = -10
            Q[state[0], state[1], action] += ALPHA * \
                (reward + GAMMA * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], action])
            Q[state[0], state[1], action] -= ALPHA * \
                (Q[state[0], state[1], action] - reward - GAMMA * np.max(Q[next_state[0], next_state[1]]))
            total_reward += reward
            state = next_state
        rewards.append(total_reward)
    return rewards


def evaluate_performance():
    rewards_q_learning = q_learning()
    rewards_double_q_learning = double_q_learning()
    rewards_self_correcting_q_learning = self_correcting_q_learning()

    plt.plot(range(NUM_EPISODES), rewards_q_learning, label='Q-Learning')
    plt.plot(range(NUM_EPISODES), rewards_double_q_learning, label='Double Q-Learning')
    plt.plot(range(NUM_EPISODES), rewards_self_correcting_q_learning, label='Self-Correcting Q-Learning')
    plt.xlabel('Episodes')
    plt.ylabel('Total Rewards')
    plt.title('Performance Comparison')
    plt.legend()
    plt.show()


def transfer_learning():
    # Implement transfer learning experiments
    pass


def main():
    evaluate_performance()
    transfer_learning()


if __name__ == "__main__":
    main()