ğŸ“˜ ANN_Sentiment_Analysis.ipynb:
**********************************************************************************************************************************
This notebook explores a series of deep learning models for sentiment analysis, focusing on progressively refining architectures and evaluating their performance. The journey spans from simple RNNs to more advanced recurrent structures and tokenization techniques using BERT.

ğŸ”§ 1. Initial RNN Model with ReLU Activation
â€¢	Architecture: Embedding â†’ SimpleRNN(32, activation='relu') â†’ Dense(softmax)
â€¢	Training: 5 epochs, categorical cross-entropy, Adam optimizer
â€¢	Evaluation:
  o	Tracked training time
  o	Plotted training and validation accuracy/loss

ğŸ” 2. Deep RNN Architecture
â€¢	Stacked RNN Layers:
  o	SimpleRNN(64, return_sequences=True) â†’ SimpleRNN(32, return_sequences=True) â†’ SimpleRNN(16)
  o	Added Dropout(0.2)
â€¢	Improved Learning Capacity for sequential dependencies
â€¢	Evaluated with similar metrics and time tracking

ğŸ§  3. Replacing RNN with LSTM
â€¢	Layer Stack:
  o	LSTM(64, return_sequences=True) â†’ LSTM(32, return_sequences=True) â†’ LSTM(16)
  o	Used Dropout(0.5)
â€¢	Rationale: LSTM improves long-term memory retention over RNN
â€¢	Results: Better validation accuracy, more stable learning curves

âš¡ 4. GRU-Based Architecture
â€¢	Replaced LSTM with GRU:
  o	GRU(64) â†’ GRU(32) â†’ GRU(16)
  o	Added Dropout(0.2)
â€¢	Benefits: Computationally efficient with competitive performance

ğŸ”„ 5. Bidirectional GRUs
â€¢	Introduced Bidirectional wrappers:
  o	Bidirectional(GRU(64)) â†’ Bidirectional(GRU(32)) â†’ Bidirectional(GRU(16))
â€¢	Goal: Capture context in both forward and backward directions
â€¢	Observation: Enhanced model understanding of sentiment semantics

ğŸ§ª 6. BERT Tokenization and Input Pipeline
â€¢	Used Hugging Face imdb dataset
â€¢	Tokenized with BertTokenizer (bert-base-uncased)
â€¢	Inputs truncated/padded to max length = 128
â€¢	Encoded labels using LabelEncoder
â€¢	Train/test split using train_test_split

âš™ï¸ 7. Custom Lightweight LSTM with Mixed Precision
â€¢	Architecture:
  o	Embedding â†’ Bidirectional(LSTM(32)) â†’ Bidirectional(LSTM(16)) â†’ Dropout(0.5) â†’ Dense(8, relu) â†’ Dense(softmax)

â€¢	Mixed Precision Training:
  o 	Enabled with tf.keras.mixed_precision
  o	Improved training speed and memory efficiency
â€¢	Training: 3 epochs, binary_crossentropy loss, 20% validation split
â€¢	Visualization: Accuracy and loss plots
â€¢	Evaluation: Measured final test accuracy and loss

ğŸ“Š Performance Tracking
Across all models:
	ğŸ“ˆ Plotted training & validation metrics
	â±ï¸ Measured total training time
	ğŸ“‰ Evaluated final model loss and accuracy on the test set
