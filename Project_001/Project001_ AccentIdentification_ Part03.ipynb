{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ASePDhZwKjT",
        "outputId": "7b16aa78-c796-42ac-875b-3ce4aa0c591d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras import layers, regularizers, optimizers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, Bidirectional,  MultiHeadAttention, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from pydub import AudioSegment\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n"
      ],
      "metadata": {
        "id": "iz6L3UqMWePX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load data from folder\n",
        "def load_data_from_folder(folder):\n",
        "    labels = []\n",
        "    data = []\n",
        "\n",
        "    for subdir, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.endswith(\".npy\"):\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                label = os.path.basename(subdir)  # Use the parent folder name as the label\n",
        "                labels.append(label)\n",
        "                data.append(np.load(file_path))\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Specify the root folder containing subdirectories with .npy files\n",
        "root_folder = \"/content/drive/MyDrive/MFCC_Features\"\n",
        "\n",
        "# Load data from subdirectories\n",
        "data, labels = load_data_from_folder(root_folder)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "encoded_labels = to_categorical(encoded_labels, num_classes=num_classes)\n",
        "\n",
        "# Split the data into training and testing sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
        ")\n",
        "\n",
        "# Data normalization using TensorFlow preprocessing layers\n",
        "normalization_layer = preprocessing.Normalization()\n",
        "normalization_layer.adapt(X_train)  # Compute mean and variance based on training data\n",
        "X_train_normalized = normalization_layer(X_train)\n",
        "X_test_normalized = normalization_layer(X_test)\n",
        "\n",
        "# Data augmentation using TensorFlow preprocessing layers\n",
        "data_augmentation = Sequential([\n",
        "    preprocessing.Rescaling(1./255),  # Normalize pixel values to [0,1]\n",
        "    preprocessing.RandomFlip(\"horizontal\"),\n",
        "    preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "X_train_augmented = data_augmentation(X_train_normalized)\n",
        "X_test_augmented = data_augmentation(X_test_normalized)\n",
        "\n",
        "# Compute class weights for handling class imbalance\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(encoded_labels.argmax(axis=1)), y=encoded_labels.argmax(axis=1))\n",
        "class_weight_dict = dict(zip(np.unique(encoded_labels.argmax(axis=1)), class_weights))\n"
      ],
      "metadata": {
        "id": "VM1ynK4t1gfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STARTING MFFC Feature TRAINING FROM HERE"
      ],
      "metadata": {
        "id": "8bkAw6HUjmzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model with added layers for better performance\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(X_train_augmented.shape[1], X_train_augmented.shape[2])))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Use the Adam optimizer for better convergence\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compile the model with categorical_crossentropy loss and accuracy metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model with a more expressive variable name for clarity\n",
        "epochs = 10\n",
        "batch_size = 22\n",
        "validation_data = (X_test_augmented, y_test)\n",
        "\n",
        "history = model.fit(X_train_augmented, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gALTmJ2jukSH",
        "outputId": "509ca444-fca2-43c1-ec1c-fc5eb6bb4220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 6s 36ms/step - loss: 2.6989 - accuracy: 0.4086 - val_loss: 2.6868 - val_accuracy: 0.4345\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 2.6693 - accuracy: 0.4339 - val_loss: 2.6391 - val_accuracy: 0.4345\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 1s 19ms/step - loss: 2.5637 - accuracy: 0.4339 - val_loss: 2.3713 - val_accuracy: 0.4345\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 1s 19ms/step - loss: 2.2138 - accuracy: 0.4339 - val_loss: 2.0989 - val_accuracy: 0.4345\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 2.1442 - accuracy: 0.4339 - val_loss: 2.0897 - val_accuracy: 0.4345\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 2.1285 - accuracy: 0.4339 - val_loss: 2.0846 - val_accuracy: 0.4345\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 2.1494 - accuracy: 0.4321 - val_loss: 2.0841 - val_accuracy: 0.4345\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 2.1361 - accuracy: 0.4339 - val_loss: 2.0829 - val_accuracy: 0.4345\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 2.1269 - accuracy: 0.4339 - val_loss: 2.0840 - val_accuracy: 0.4345\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 2.1157 - accuracy: 0.4330 - val_loss: 2.0827 - val_accuracy: 0.4345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model with added bidirectional layers for better performance\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_augmented.shape[1], X_train_augmented.shape[2])))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Use the Adam optimizer for better convergence\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with categorical_crossentropy loss and accuracy metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model with a more expressive variable name for clarity\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "validation_data = (X_test_augmented, y_test)\n",
        "\n",
        "history = model.fit(X_train_augmented, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xnHXkmL3pSQ",
        "outputId": "3d7d8618-92f3-4e71-e608-89935b05b443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "34/34 [==============================] - 13s 93ms/step - loss: 2.4202 - accuracy: 0.4189 - val_loss: 2.1788 - val_accuracy: 0.4345\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 2.1618 - accuracy: 0.4339 - val_loss: 2.0779 - val_accuracy: 0.4345\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 2.1327 - accuracy: 0.4339 - val_loss: 2.0740 - val_accuracy: 0.4345\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 2.1305 - accuracy: 0.4339 - val_loss: 2.0745 - val_accuracy: 0.4345\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 2s 73ms/step - loss: 2.1219 - accuracy: 0.4339 - val_loss: 2.0842 - val_accuracy: 0.4345\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 3s 75ms/step - loss: 2.1122 - accuracy: 0.4339 - val_loss: 2.0913 - val_accuracy: 0.4345\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 3s 76ms/step - loss: 2.1061 - accuracy: 0.4339 - val_loss: 2.0990 - val_accuracy: 0.4345\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 2.1210 - accuracy: 0.4339 - val_loss: 2.0829 - val_accuracy: 0.4345\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 2.1053 - accuracy: 0.4339 - val_loss: 2.0747 - val_accuracy: 0.4345\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 2.1202 - accuracy: 0.4339 - val_loss: 2.0748 - val_accuracy: 0.4345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the Dropout\n",
        "# Build the LSTM model with added bidirectional layers for better performance\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_augmented.shape[1], X_train_augmented.shape[2])))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Use the Adam optimizer for better convergence\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with categorical_crossentropy loss and accuracy metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model with a more expressive variable name for clarity\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "validation_data = (X_test_augmented, y_test)\n",
        "\n",
        "history = model.fit(X_train_augmented, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McozR8oS4E3s",
        "outputId": "60871b19-3567-4227-fcf4-ed422a8919fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "34/34 [==============================] - 14s 89ms/step - loss: 2.3617 - accuracy: 0.4142 - val_loss: 2.1452 - val_accuracy: 0.4157\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 2.0948 - accuracy: 0.4386 - val_loss: 2.1455 - val_accuracy: 0.4157\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 2.0913 - accuracy: 0.4386 - val_loss: 2.1419 - val_accuracy: 0.4157\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 2.0967 - accuracy: 0.4386 - val_loss: 2.1388 - val_accuracy: 0.4157\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 2.0967 - accuracy: 0.4386 - val_loss: 2.1418 - val_accuracy: 0.4157\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 2s 55ms/step - loss: 2.0890 - accuracy: 0.4386 - val_loss: 2.1459 - val_accuracy: 0.4157\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 2s 69ms/step - loss: 2.0897 - accuracy: 0.4386 - val_loss: 2.1356 - val_accuracy: 0.4157\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 2s 70ms/step - loss: 2.0815 - accuracy: 0.4386 - val_loss: 2.1368 - val_accuracy: 0.4157\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 2s 67ms/step - loss: 2.0868 - accuracy: 0.4386 - val_loss: 2.1399 - val_accuracy: 0.4157\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 2.0767 - accuracy: 0.4386 - val_loss: 2.1384 - val_accuracy: 0.4157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Bidirectional, Dense, Dropout, LSTM, Attention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(X_train_augmented.shape[1], X_train_augmented.shape[2]))\n",
        "\n",
        "# Bidirectional LSTM with attention\n",
        "lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(input_layer)\n",
        "attention = Attention()([lstm_layer, lstm_layer])\n",
        "dropout_1 = Dropout(0.5)(attention)\n",
        "\n",
        "lstm_layer_2 = Bidirectional(LSTM(64))(dropout_1)\n",
        "attention_2 = Attention()([lstm_layer_2, lstm_layer_2])\n",
        "dropout_2 = Dropout(0.5)(attention_2)\n",
        "\n",
        "# Dense layers\n",
        "dense_1 = Dense(128, activation='relu')(dropout_2)\n",
        "dropout_3 = Dropout(0.2)(dense_1)\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(dropout_3)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Use the Adam optimizer for better convergence\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with categorical_crossentropy loss and accuracy metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "validation_data = (X_test_augmented, y_test)\n",
        "\n",
        "history = model.fit(X_train_augmented, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssr_qGAk4ezv",
        "outputId": "a47ac4c4-2965-47d9-8ee5-a7a187eb0f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "34/34 [==============================] - 15s 95ms/step - loss: 2.3684 - accuracy: 0.4180 - val_loss: 2.0951 - val_accuracy: 0.4345\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 2.1158 - accuracy: 0.4339 - val_loss: 2.0817 - val_accuracy: 0.4345\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 2.0826 - accuracy: 0.4339 - val_loss: 2.0838 - val_accuracy: 0.4345\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 2.1151 - accuracy: 0.4339 - val_loss: 2.1019 - val_accuracy: 0.4345\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 2.0969 - accuracy: 0.4339 - val_loss: 2.0784 - val_accuracy: 0.4345\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 2s 56ms/step - loss: 2.0989 - accuracy: 0.4339 - val_loss: 2.0913 - val_accuracy: 0.4345\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 2s 72ms/step - loss: 2.0999 - accuracy: 0.4339 - val_loss: 2.0744 - val_accuracy: 0.4345\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 3s 77ms/step - loss: 2.0952 - accuracy: 0.4339 - val_loss: 2.0778 - val_accuracy: 0.4345\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 2s 64ms/step - loss: 2.0919 - accuracy: 0.4339 - val_loss: 2.0941 - val_accuracy: 0.4345\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 2.0916 - accuracy: 0.4339 - val_loss: 2.0732 - val_accuracy: 0.4345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Use the Adam optimizer for better convergence\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "validation_data = (X_test, y_test)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print the combined accuracy result\n",
        "print(f\"Combined Accuracy on Test Set: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suKDSnnz9Qs-",
        "outputId": "4278cb5d-2c7f-4fe7-ac2f-c17b5b10a81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "34/34 [==============================] - 13s 78ms/step - loss: 2.2576 - accuracy: 0.3889 - val_loss: 2.0876 - val_accuracy: 0.4345\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 2.0946 - accuracy: 0.4321 - val_loss: 2.0533 - val_accuracy: 0.4345\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 2.0517 - accuracy: 0.4358 - val_loss: 2.0481 - val_accuracy: 0.4345\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 2.0131 - accuracy: 0.4330 - val_loss: 2.0340 - val_accuracy: 0.4345\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 1.9681 - accuracy: 0.4358 - val_loss: 2.0880 - val_accuracy: 0.4157\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 1.9362 - accuracy: 0.4377 - val_loss: 2.1442 - val_accuracy: 0.4307\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 1.9314 - accuracy: 0.4508 - val_loss: 2.1928 - val_accuracy: 0.3933\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 1s 26ms/step - loss: 1.9154 - accuracy: 0.4414 - val_loss: 2.1073 - val_accuracy: 0.4120\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 1.8628 - accuracy: 0.4489 - val_loss: 2.1201 - val_accuracy: 0.4045\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 1.7912 - accuracy: 0.4602 - val_loss: 2.1385 - val_accuracy: 0.4045\n",
            "Combined Accuracy on Test Set: 40.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming you have a dataset X_train and y_train\n",
        "# X_train should be a 2D array with shape (number_of_samples, input_size)\n",
        "# y_train should be a 2D array with shape (number_of_samples, output_size)\n",
        "\n",
        "# Define the architecture parameters\n",
        "input_size = 25  # Size of input layer\n",
        "hidden_size = 100  # Size of hidden layers\n",
        "output_size = 25  # Size of output layer\n",
        "\n",
        "# Generate synthetic data (replace this with your actual data loading)\n",
        "X_train = np.random.randn(1000, input_size)\n",
        "y_train = np.random.randn(1000, output_size)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the input data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(input_size,)),\n",
        "    tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=False),\n",
        "    tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=False),\n",
        "    tf.keras.layers.Dense(output_size, activation=None, use_bias=False)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "history = model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print the combined accuracy result\n",
        "print(f\"Combined Accuracy on Test Set: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJUKhg7LAMnP",
        "outputId": "ba3c43a6-2abb-4093-fffb-9df6d1f41862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_46 (Dense)            (None, 100)               2500      \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 100)               10000     \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 25)                2500      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15000 (58.59 KB)\n",
            "Trainable params: 15000 (58.59 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 1s 10ms/step - loss: 1.0633 - mae: 0.8218 - val_loss: 1.0122 - val_mae: 0.8002\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.9993 - mae: 0.7971 - val_loss: 1.0012 - val_mae: 0.7953\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.9778 - mae: 0.7882 - val_loss: 1.0001 - val_mae: 0.7951\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.9622 - mae: 0.7816 - val_loss: 1.0016 - val_mae: 0.7954\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.9494 - mae: 0.7763 - val_loss: 1.0052 - val_mae: 0.7971\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.9376 - mae: 0.7717 - val_loss: 1.0086 - val_mae: 0.7984\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.9263 - mae: 0.7670 - val_loss: 1.0117 - val_mae: 0.7996\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.9159 - mae: 0.7626 - val_loss: 1.0162 - val_mae: 0.8014\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.9049 - mae: 0.7579 - val_loss: 1.0215 - val_mae: 0.8033\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.8944 - mae: 0.7533 - val_loss: 1.0251 - val_mae: 0.8047\n",
            "Combined Accuracy on Test Set: 80.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_frames = 100  # Replace with the actual number of frames in your data\n",
        "num_mfcc_features = 13  # Replace with the actual number of MFCC features extracted per frame\n",
        "num_short_term_features = 20  # Replace with the actual number of short-term features in your data\n",
        "num_classes = 15"
      ],
      "metadata": {
        "id": "mKvsyhUV2ZDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, LSTM, Dense, concatenate, Bidirectional, GRU, Reshape\n",
        "\n",
        "# Define the input shape for MFCC features\n",
        "mfcc_input = Input(shape=(num_frames, num_mfcc_features, 1), name='mfcc_input')\n",
        "\n",
        "# CNN for processing MFCC features\n",
        "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(mfcc_input)\n",
        "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "# Add more Conv2D and MaxPooling layers as needed\n",
        "\n",
        "# Reshape the output of the CNN to a 3D tensor\n",
        "reshaped_cnn_output = Reshape((-1, 32))(maxpool1)\n",
        "\n",
        "# LSTM for extracting long-term features\n",
        "lstm_output = LSTM(64)(reshaped_cnn_output)\n",
        "\n",
        "# Define the input shape for short-term features\n",
        "short_term_input = Input(shape=(num_frames, num_short_term_features), name='short_term_input')\n",
        "\n",
        "# RNN for processing short-term features\n",
        "rnn_output = Bidirectional(GRU(32))(short_term_input)\n",
        "\n",
        "# Concatenate LSTM and RNN outputs\n",
        "merged_output = concatenate([lstm_output, rnn_output])\n",
        "\n",
        "# DNN for processing long-term features\n",
        "dnn_output = Dense(64, activation='relu')(merged_output)\n",
        "# Add more Dense layers as needed\n",
        "\n",
        "# Probabilistic fusion algorithm (example: simple concatenation)\n",
        "fusion_output = concatenate([lstm_output, dnn_output])\n",
        "\n",
        "# Reshape the fusion_output to a 3D tensor\n",
        "reshaped_fusion_output = Reshape((1, -1))(fusion_output)\n",
        "\n",
        "# RNN for combining results\n",
        "final_rnn_output = GRU(32, return_sequences=True)(reshaped_fusion_output)\n",
        "\n",
        "# Output layer\n",
        "num_classes = 10  # Replace with the actual number of classes\n",
        "output = Dense(num_classes, activation='softmax')(final_rnn_output)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[mfcc_input, short_term_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "myaNrJk22AQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b1163f-94ec-4b31-f173-dac1c2c792bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " mfcc_input (InputLayer)     [(None, 100, 13, 1)]         0         []                            \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 98, 11, 32)           320       ['mfcc_input[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 49, 5, 32)            0         ['conv2d_7[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " reshape_7 (Reshape)         (None, 245, 32)              0         ['max_pooling2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " short_term_input (InputLay  [(None, 100, 20)]            0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)               (None, 64)                   24832     ['reshape_7[0][0]']           \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirecti  (None, 64)                   10368     ['short_term_input[0][0]']    \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate  (None, 128)                  0         ['lstm_7[0][0]',              \n",
            " )                                                                   'bidirectional_4[0][0]']     \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 64)                   8256      ['concatenate_8[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate  (None, 128)                  0         ['lstm_7[0][0]',              \n",
            " )                                                                   'dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_8 (Reshape)         (None, 1, 128)               0         ['concatenate_9[0][0]']       \n",
            "                                                                                                  \n",
            " gru_12 (GRU)                (None, 1, 32)                15552     ['reshape_8[0][0]']           \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 1, 10)                330       ['gru_12[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 59658 (233.04 KB)\n",
            "Trainable params: 59658 (233.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}