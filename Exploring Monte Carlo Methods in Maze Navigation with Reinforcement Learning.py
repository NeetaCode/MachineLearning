# -*- coding: utf-8 -*-
"""RL HOMEWORK 1 CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BS5KdZCN1f9zbXZijNh0fExIRGVQ2kh_

Random Policy for 5 * 5 maze for 100 steps
"""

import random

# Define constants
ROWS = 5
COLS = 5
START_STATE = (0, 0)
GOAL_STATE = (4, 4)
ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']
REWARDS = {'GOAL': 10, 'OTHER': -1}  # Reward for reaching the goal and for other states

# Define the maze
maze = [[' ' for _ in range(COLS)] for _ in range(ROWS)]
maze[0][0] = 'S'  # Start state
maze[4][4] = 'G'  # Goal state
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]  # Barrier positions
for barrier in barriers:
    maze[barrier[0]][barrier[1]] = 'X'

# Define a function to initialize a random policy
def initialize_random_policy():
    policy = {}
    for i in range(ROWS):
        for j in range(COLS):
            if maze[i][j] != 'X':  # Ignore barriers
                state = (i, j)
                policy[state] = random.choice(ACTIONS)  # Randomly choose an action
    return policy

# Initialize the random policy
policy = initialize_random_policy()

# Define a function to take an action in the maze according to the policy
def take_action(state, action):
    row, col = state
    if action == 'UP':
        row = max(0, row - 1)
    elif action == 'DOWN':
        row = min(ROWS - 1, row + 1)
    elif action == 'LEFT':
        col = max(0, col - 1)
    elif action == 'RIGHT':
        col = min(COLS - 1, col + 1)
    return (row, col)

# Define a function to get the reward for a given state
def get_reward(state):
    if state == GOAL_STATE:
        return REWARDS['GOAL']
    else:
        return REWARDS['OTHER']

# Define a function to simulate agent movement in the maze according to the random policy
def simulate_movement(max_steps=100):
    current_state = START_STATE
    total_reward = 0
    steps = 0
    while current_state != GOAL_STATE and steps < max_steps:
        action = policy[current_state]
        next_state = take_action(current_state, action)
        reward = get_reward(next_state)
        total_reward += reward
        print(f"Current State: {current_state}, Action: {action}, Next State: {next_state}, Reward: {reward}")
        current_state = next_state
        steps += 1
    if current_state == GOAL_STATE:
        print("Agent reached the goal state!")
    else:
        print("Agent failed to reach the goal state within the maximum number of steps.")
    return total_reward

# Simulate agent movement in the maze according to the random policy
total_reward = simulate_movement(max_steps=100)
print(f"Total Reward: {total_reward}")

"""Deterministic Policy for 5 * 5 maze"""

# Define constants
ROWS = 5
COLS = 5
START_STATE = (0, 0)
GOAL_STATE = (4, 4)
MAX_STEPS = 15  # Adjust the maximum number of steps as needed

# Define the maze
maze = [[' ' for _ in range(COLS)] for _ in range(ROWS)]
maze[0][0] = 'S'  # Start state
maze[4][4] = 'G'  # Goal state
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]  # Barrier positions
for barrier in barriers:
    maze[barrier[0]][barrier[1]] = 'X'

# Define a function to take an action in the maze
def take_action(state, action):
    row, col = state
    if action == 'UP':
        row = max(0, row - 1)
    elif action == 'DOWN':
        row = min(ROWS - 1, row + 1)
    elif action == 'LEFT':
        col = max(0, col - 1)
    elif action == 'RIGHT':
        col = min(COLS - 1, col + 1)
    return (row, col)

# Define rewards for each state transition
rewards = {
    ((0, 0), 'RIGHT'): -1, ((0, 1), 'RIGHT'): -1, ((0, 2), 'RIGHT'): -1, ((0, 3), 'RIGHT'): -1, ((0, 4), 'DOWN'): -1,
    ((1, 0), 'DOWN'): -1, ((1, 1), 'DOWN'): -1, ((1, 2), 'RIGHT'): -1, ((1, 3), 'RIGHT'): -1, ((1, 4), 'DOWN'): -1,
    ((2, 0), 'DOWN'): -1, ((2, 1), 'DOWN'): -1, ((2, 2), 'DOWN'): -1, ((2, 3), 'DOWN'): -1, ((2, 4), 'DOWN'): -1,
    ((3, 0), 'RIGHT'): -1, ((3, 1), 'RIGHT'): -1, ((3, 2), 'RIGHT'): -1, ((3, 3), 'RIGHT'): -1, ((3, 4), 'DOWN'): -1,
    ((4, 0), 'RIGHT'): -1, ((4, 1), 'RIGHT'): -1, ((4, 2), 'RIGHT'): -1, ((4, 3), 'RIGHT'): -1, ((4, 4), None): 100  # Increased goal state reward
}

# Define a deterministic policy to guide the agent from start to goal
deterministic_policy = {
    (0, 0): 'RIGHT', (0, 1): 'RIGHT', (0, 2): 'RIGHT', (0, 3): 'RIGHT', (0, 4): 'DOWN',
    (1, 0): 'DOWN', (1, 1): 'DOWN', (1, 2): 'RIGHT', (1, 3): 'RIGHT', (1, 4): 'DOWN',
    (2, 0): 'DOWN', (2, 1): 'DOWN', (2, 2): 'DOWN', (2, 3): 'DOWN', (2, 4): 'DOWN',
    (3, 0): 'RIGHT', (3, 1): 'RIGHT', (3, 2): 'RIGHT', (3, 3): 'RIGHT', (3, 4): 'DOWN',
    (4, 0): 'RIGHT', (4, 1): 'RIGHT', (4, 2): 'RIGHT', (4, 3): 'RIGHT', (4, 4): None  # Goal state
}

# Define a function to simulate agent movement in the maze according to the deterministic policy
def simulate_deterministic_movement():
    current_state = START_STATE
    total_reward = 0
    steps = 0
    while current_state != GOAL_STATE and steps < MAX_STEPS:
        action = deterministic_policy[current_state]
        if action is None:
            break  # Goal state reached
        next_state = take_action(current_state, action)
        reward = rewards.get((current_state, action), 0)  # Get reward for the state transition
        total_reward += reward
        print(f"Current State: {current_state}, Action: {action}, Next State: {next_state}, Reward: {reward}")
        current_state = next_state
        steps += 1

    if current_state == GOAL_STATE:
        # Add the reward for reaching the goal state after the loop exits
        total_reward += rewards.get((current_state, None), 0)
        print("Agent reached the goal state!")
    else:
        print("Agent failed to reach the goal state within the maximum number of steps.")

    return total_reward

# Simulate agent movement in the maze according to the deterministic policy
total_reward = simulate_deterministic_movement()
print(f"Total Reward: {total_reward}")

"""5*5 maze based on reward sturcture including the barriers"""

import numpy as np
import random

# Define constants
ROWS = 5
COLS = 5
START_STATE = (0, 0)
GOAL_STATE = (4, 4)
ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']
EPSILON = 0.1  # Exploration rate

# Define the maze
maze = [[' ' for _ in range(COLS + 2)] for _ in range(ROWS + 2)]  # Add borders
for i in range(1, ROWS + 1):
    for j in range(1, COLS + 1):
        maze[i][j] = ' '  # Initialize inner maze cells to empty space
maze[START_STATE[0] + 1][START_STATE[1] + 1] = 'S'  # Start state
maze[GOAL_STATE[0] + 1][GOAL_STATE[1] + 1] = 'G'  # Goal state
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]  # Barrier positions
for barrier in barriers:
    maze[barrier[0] + 1][barrier[1] + 1] = 'X'  # Barrier positions

# Define rewards for each state transition
rewards = {}
for i in range(ROWS):
    for j in range(COLS):
        for action in ACTIONS:
            if maze[i + 1][j + 1] == 'X':  # Check if the current state is a barrier
                rewards[(i, j), action] = -10  # High penalty for transitioning to a barrier state
            else:
                rewards[(i, j), action] = -1  # Small penalty for other actions

# Reward for reaching the goal state
rewards[(4, 4), None] = 10000

# Initialize equi-random policy
policy = np.ones((ROWS, COLS, len(ACTIONS))) / len(ACTIONS)

# Define a function to take an action in the maze
def take_action(state, action):
    row, col = state
    if action == 'UP':
        row = max(0, row - 1)
    elif action == 'DOWN':
        row = min(ROWS - 1, row + 1)
    elif action == 'LEFT':
        col = max(0, col - 1)
    elif action == 'RIGHT':
        col = min(COLS - 1, col + 1)
    return (row, col)

# Define a function to simulate an episode using the current policy
def simulate_episode():
    episode = []
    current_state = START_STATE
    total_reward = 0
    while current_state != GOAL_STATE:
        # Normalize probabilities
        normalized_probs = policy[current_state] / np.sum(policy[current_state])
        action = np.random.choice(ACTIONS, p=normalized_probs)
        next_state = take_action(current_state, action)
        reward = rewards.get((current_state, action), 0)  # Get reward for the state transition
        total_reward += reward
        episode.append((current_state, action, reward))
        current_state = next_state
    return total_reward, episode  # Return both total_reward and episode

# Monte Carlo Policy Iteration
policy_stable = False
while not policy_stable:
    policy_stable = True
    returns_sum = np.zeros((ROWS, COLS, len(ACTIONS)))
    returns_count = np.zeros((ROWS, COLS, len(ACTIONS)))
    for _ in range(100):  # Simulate episodes in batches
        total_reward, episode = simulate_episode()  # Simulate a single episode and get episode
        G = 0
        states_actions_visited = set()
        for t in range(len(episode) - 1, -1, -1):  # Calculate returns
            state, action, reward = episode[t]
            G += reward
            if (state, action) not in states_actions_visited:
                states_actions_visited.add((state, action))
                action_index = ACTIONS.index(action)
                if maze[state[0] + 1][state[1] + 1] != 'X':  # Update policy only if not going into a barrier
                    returns_sum[state][action_index] += G
                    returns_count[state][action_index] += 1
                    best_action = np.nanargmax(returns_sum[state] / returns_count[state])
                    if policy[state][best_action] < 1 - EPSILON:
                        policy_stable = False
                    policy[state] = np.eye(len(ACTIONS))[best_action]
                else:
                    # If the action leads to a barrier, set the probability of that action to 0
                    policy[state] /= np.sum(policy[state])  # Normalize policy
                    policy[state][action_index] = 0

# Update the maze with actions of learned policy
for i in range(ROWS):
    for j in range(COLS):
        if maze[i + 1][j + 1] == ' ':
            action = ACTIONS[np.argmax(policy[i, j])]
            if action == 'UP':
                maze[i + 1][j + 1] = '^'
            elif action == 'DOWN':
                maze[i + 1][j + 1] = 'v'
            elif action == 'LEFT':
                maze[i + 1][j + 1] = '<'
            elif action == 'RIGHT':
                maze[i + 1][j + 1] = '>'

# Print the maze with borders
print("\nMaze:")
for row in maze:
    print("".join(row))

##SOLUTIONS EXPLORING OPTIONS BUT HITTING THE BARRIERS

import numpy as np
import random

# Define constants
ROWS = 5
COLS = 5
START = (0, 0)
GOAL = (4, 4)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.1
GAMMA = 0.9
EPISODES = 10000

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS:
            episode.append((state, action))
            state = next_state

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
    path.append(next_state)
    state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

#SINGLE BEST SOLUTIONS EXPLORING OPTIONS WITHOUT HITTING THE BARRIERS

import numpy as np
import random

# Define constants
ROWS = 5
COLS = 5
START = (0, 0)
GOAL = (4, 4)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.3  # Adjust exploration rate
GAMMA = 0.9
EPISODES = 10000

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

#MULTIPLE BEST SOLUTIONS EXPLORING OPTIONS WITHOUT HITTING THE BARRIERS

import numpy as np
import random

# Define constants
ROWS = 5
COLS = 5
START = (0, 0)
GOAL = (4, 4)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

#Code for Printing all actions taken in all sates

import numpy as np
import random

# Define constants
ROWS = 5
COLS = 5
START = (0, 0)
GOAL = (4, 4)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.3  # Adjust exploration rate
GAMMA = 0.9
EPISODES = 10000

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

print("Actions Taken in All States:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) == START:
            print("S", end=" ")
        elif (row, col) == GOAL:
            print("G", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            action_idx = np.argmax(Q[(row, col)])
            arrow = ARROWS[action_idx]
            print(arrow, end=" ")
    print()

import numpy as np
import random

# Define constants
ROWS = 5
COLS = 5
START = (0, 0)
GOAL = (4, 4)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Check if policy is stable
stable_policy = True
for row in range(ROWS):
    for col in range(COLS):
        if maze[row][col] != -10 and (row, col) != GOAL:
            max_action = np.argmax(Q[(row, col)])
            if Q[(row, col)][max_action] == 0:
                stable_policy = False

print("Stable Policy:", stable_policy)

# Execute learned policy
state = START
path = [state]
total_reward = 0  # Initialize total reward
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
        if state == GOAL:
            total_reward += goal_reward  # Add goal reward
        else:
            total_reward += maze[state]  # Add reward obtained in this cell
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

print("Total Reward:", total_reward)

import numpy as np
import random

# Define constants
ROWS = 5
COLS = 5
START = (0, 0)
GOAL = (4, 4)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Print learned policy for optimal path only
print("Learned Plocy:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            if (row, col) == START:
                print("S", end=" ")  # start state
            elif (row, col) == GOAL:
                print("G", end=" ")  # goal state
            elif (row, col) in barriers:
                print("#", end=" ")  # barrier
            else:
                action = np.argmax(Q[(row, col)])
                if action == 0:
                    print("→", end=" ")  # right
                elif action == 1:
                    print("←", end=" ")  # left
                elif action == 2:
                    print("↓", end=" ")  # down
                elif action == 3:
                    print("↑", end=" ")  # up
        else:
            print(".", end=" ")  # empty cell
    print()


# Execute learned policy
state = START
path = [state]
total_reward = 0  # Initialize total reward
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
        if state == GOAL:
            total_reward += goal_reward  # Add goal reward
        else:
            total_reward += maze[state]  # Add reward obtained in this cell
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

print("Total Reward:", total_reward)

import numpy as np
import random

# Define constants
ROWS = 5
COLS = 5
START = (0, 0)
GOAL = (4, 4)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Print learned policy for optimal path only
print("Learned Plocy:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            if (row, col) == START:
                print("S", end=" ")  # start state
            elif (row, col) == GOAL:
                print("G", end=" ")  # goal state
            elif (row, col) in barriers:
                print("#", end=" ")  # barrier
            else:
                action = np.argmax(Q[(row, col)])
                if action == 0:
                    print("→", end=" ")  # right
                elif action == 1:
                    print("←", end=" ")  # left
                elif action == 2:
                    print("↓", end=" ")  # down
                elif action == 3:
                    print("↑", end=" ")  # up
        else:
            print(".", end=" ")  # empty cell
    print()

# Print execution time
print("Execution Time:", execution_time, "seconds")

# Check if policy is stable
stable_policy = True
for row in range(ROWS):
    for col in range(COLS):
        if maze[row][col] != -10 and (row, col) != GOAL:
            max_action = np.argmax(Q[(row, col)])
            if Q[(row, col)][max_action] == 0:
                stable_policy = False

print("Stable Policy:", stable_policy)

# Execute learned policy
state = START
path = [state]
total_reward = 0  # Initialize total reward
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
        if state == GOAL:
            total_reward += goal_reward  # Add goal reward
        else:
            total_reward += maze[state]  # Add reward obtained in this cell
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

print("Total Reward:", total_reward)

"""All mazes included"""

import numpy as np
import random

# Define constants
ROWS = 10
COLS = 10
START = (0, 0)
GOAL = (9, 9)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with random barriers
maze = np.zeros((ROWS, COLS))
num_barriers = random.randint(10, 30)  # Random number of barriers
barriers = []
for _ in range(num_barriers):
    barrier = (random.randint(0, ROWS - 1), random.randint(0, COLS - 1))
    if barrier != START and barrier != GOAL:
        barriers.append(barrier)
        maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

import numpy as np
import random

# Define constants
ROWS = 10
COLS = 10
START = (0, 0)
GOAL = (9, 9)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.3  # Adjust exploration rate
GAMMA = 0.9
EPISODES = 10000

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(0, 2), (0, 6), (2, 2), (2, 5), (2, 6), (3, 8), (4, 2), (5, 6), (6, 7), (6, 8), (9, 5), (9, 6)]  # Additional barriers for 10x10 maze
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

print("Actions Taken in All States:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) == START:
            print("S", end=" ")
        elif (row, col) == GOAL:
            print("G", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            action_idx = np.argmax(Q[(row, col)])
            arrow = ['→', '←', '↓', '↑'][action_idx]  # Arrow symbols corresponding to actions
            print(arrow, end=" ")
    print()

import numpy as np
import random
import time

# Define constants
ROWS = 10
COLS = 10
START = (0, 0)
GOAL = (9, 9)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with random barriers
maze = np.zeros((ROWS, COLS))
num_barriers = random.randint(10, 30)  # Random number of barriers
barriers = []
for _ in range(num_barriers):
    barrier = (random.randint(0, ROWS - 1), random.randint(0, COLS - 1))
    if barrier != START and barrier != GOAL:
        barriers.append(barrier)
        maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
start_time = time.time()
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
actions = []
while state != GOAL:
    action = np.argmax(Q[state])
    actions.append(action)
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        actions.append(action)
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

# Print actions of the learned policy for the optimal path in maze form
print("\nActions of the learned policy for the optimal path in maze form:")
actions_mapping = {0: "→", 1: "←", 2: "↓", 3: "↑"}
action_index = 0
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print(actions_mapping[actions[action_index]], end=" ")
            action_index += 1
        else:
            print("  ", end="")  # Empty cell with double space
    print()  # Move to the next row

# Print execution time
print("\nExecution Time:", time.time() - start_time, "seconds")

# Check if the policy is stable
stable_policy = True
for state in path:
    if np.argmax(Q[state]) != action:
        stable_policy = False
        break
print("Stable Policy:", stable_policy)

import numpy as np
import random

# Define constants
ROWS = 20
COLS = 20
START = (0, 0)
GOAL = (19, 19)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with random barriers
maze = np.zeros((ROWS, COLS))
num_barriers = random.randint(30, 100)  # Random number of barriers
barriers = []
for _ in range(num_barriers):
    barrier = (random.randint(0, ROWS - 1), random.randint(0, COLS - 1))
    if barrier != START and barrier != GOAL:
        barriers.append(barrier)
        maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

import numpy as np
import random
import time

# Define constants
ROWS = 20
COLS = 20
START = (0, 0)
GOAL = (19, 19)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(10, i) for i in range(1, 11)] + [(15, i) for i in range(11, 20)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
start_time = time.time()
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
total_reward = 0  # Initialize total reward
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
        if state == GOAL:
            total_reward += goal_reward  # Add goal reward
        else:
            total_reward += maze[state]  # Add reward obtained in this cell
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

print("Total Reward:", total_reward)

# Print execution time
print("Execution Time:", time.time() - start_time, "seconds")

import numpy as np
import random

# Define constants
ROWS = 20
COLS = 20
START = (0, 0)
GOAL = (19, 19)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.3  # Adjust exploration rate
GAMMA = 0.9
EPISODES = 10000

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(11, 10), (12, 10), (13, 10), (14, 10), (15, 10), (16, 10), (17, 10), (18, 10), (19, 10), (15, 15), (15, 16), (15, 17), (15, 18), (16, 16), (16, 17), (16, 18), (17, 16), (17, 17), (17, 18), (18, 17), (18, 18)
]  # Additional barriers for 20x20 maze
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

print("Actions Taken in All States:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) == START:
            print("S", end=" ")
        elif (row, col) == GOAL:
            print("G", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            action_idx = np.argmax(Q[(row, col)])
            arrow = ['→', '←', '↓', '↑'][action_idx]  # Arrow symbols corresponding to actions
            print(arrow, end=" ")
    print()

import numpy as np
import random

# Define constants
ROWS = 30
COLS = 30
START = (0, 0)
GOAL = (29, 29)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.3  # Adjust exploration rate
GAMMA = 0.9
EPISODES = 10

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [
    (11, 10), (12, 10), (13, 10), (14, 10), (15, 10), (16, 10), (17, 10), (18, 10), (19, 10),
    (15, 15), (15, 16), (15, 17), (15, 18), (16, 16), (16, 17), (16, 18),
    (17, 16), (17, 17), (17, 18), (18, 17), (18, 18)
]  # Additional barriers for 30x30 maze
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

print("Actions Taken in All States:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) == START:
            print("S", end=" ")
        elif (row, col) == GOAL:
            print("G", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            action_idx = np.argmax(Q[(row, col)])
            arrow = ['→', '←', '↓', '↑'][action_idx]  # Arrow symbols corresponding to actions
            print(arrow, end=" ")
    print()

import numpy as np
import random
import time

# Define constants
ROWS = 20
COLS = 20
START = (0, 0)
GOAL = (19, 19)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(10, i) for i in range(1, 11)] + [(15, i) for i in range(11, 20)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
start_time = time.time()
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])
            # Check if all actions have zero Q-values
            if np.sum(Q[state]) == 0:
                action = random.choice(range(len(ACTIONS)))  # Choose a random action

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
total_reward = 0  # Initialize total reward
policy_actions = []  # Store learned policy actions
max_steps = ROWS * COLS  # Maximum number of steps to prevent infinite loop
steps = 0  # Initialize steps counter
while state != GOAL and steps < max_steps:
    action = np.argmax(Q[state])
    policy_actions.append(action)  # Store learned policy action
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
        if state == GOAL:
            total_reward += goal_reward  # Add goal reward
        else:
            total_reward += maze[state]  # Add reward obtained in this cell
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

    steps += 1  # Increment steps counter

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

print("Total Reward:", total_reward)

# Print learned policy action for optimal path
print("Learned Policy Action for Optimal Path:")
for action in policy_actions:
    if action == 0:
        print("→", end=" ")  # right
    elif action == 1:
        print("←", end=" ")  # left
    elif action == 2:
        print("↓", end=" ")  # down
    elif action == 3:
        print("↑", end=" ")  # up
print()

# Print execution time
print("Execution Time:", time.time() - start_time, "seconds")

import numpy as np
import random
import time

# Define constants
ROWS = 20
COLS = 20
START = (0, 0)
GOAL = (19, 19)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 20000  # Increased number of episodes

# Define maze with barriers
maze = np.zeros((ROWS, COLS))
barriers = [(10, i) for i in range(1, 11)] + [(15, i) for i in range(11, 20)]
for barrier in barriers:
    maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
start_time = time.time()
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
total_reward = 0  # Initialize total reward
learned_policy_actions = []  # Store learned policy actions for optimal path
while state != GOAL:
    action = np.argmax(Q[state])
    learned_policy_actions.append(action)  # Append learned policy action
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and maze[next_state] != -10:
        path.append(next_state)
        state = next_state
        if state == GOAL:
            total_reward += goal_reward  # Add goal reward
        else:
            total_reward += maze[state]  # Add reward obtained in this cell
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

print("Total Reward:", total_reward)

# Print learned policy action for optimal path
print("Learned Policy Action for Optimal Path:")
for i, action in enumerate(learned_policy_actions):
    if i % COLS == 0:
        print()
    if action == 0:
        print("→", end=" ")  # right
    elif action == 1:
        print("←", end=" ")  # left
    elif action == 2:
        print("↓", end=" ")  # down
    elif action == 3:
        print("↑", end=" ")  # up

# Print execution time
print("\nExecution Time:", time.time() - start_time, "seconds")

import numpy as np
import random

# Define constants
ROWS = 30
COLS = 30
START = (0, 0)
GOAL = (ROWS - 1, COLS - 1)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON = 0.5  # Increased exploration rate
GAMMA = 0.9
EPISODES = 5000  # Reduced number of episodes

# Define maze with random barriers
maze = np.zeros((ROWS, COLS))
num_barriers = random.randint(500, 1000)  # Random number of barriers
barriers = set()
for _ in range(num_barriers):
    barrier = (random.randint(0, ROWS - 1), random.randint(0, COLS - 1))
    if barrier != START and barrier != GOAL:
        barriers.add(barrier)
        maze[barrier] = -10

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-table and N-table
Q = np.zeros((ROWS, COLS, len(ACTIONS)))
N = np.zeros((ROWS, COLS, len(ACTIONS)))  # State-action visit counter

# Monte Carlo learning
for _ in range(EPISODES):
    episode = []
    state = START
    while state != GOAL:
        if random.uniform(0, 1) < EPSILON:
            action = random.choice(range(len(ACTIONS)))
        else:
            action = np.argmax(Q[state])

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and next_state not in barriers:
            episode.append((state, action))
            state = next_state
        else:
            # Penalize the agent and reset to START if it hits a barrier or outside the maze
            Q[state][action] += GAMMA * outside_penalty
            state = START

    G = goal_reward
    for step in reversed(episode):
        state, action = step
        N[state][action] += 1
        alpha = 1 / N[state][action]  # Step size
        Q[state][action] += alpha * (G - Q[state][action])
        G *= GAMMA

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    action = np.argmax(Q[state])
    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Check if the next state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and next_state not in barriers:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        action = random.choice(range(len(ACTIONS)))
        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])
        path.append(next_state)
        state = next_state

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

import numpy as np
import random
import time

# Define constants
ROWS = 30
COLS = 30
START = (0, 0)
GOAL = (ROWS - 1, COLS - 1)
ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
EPSILON_START = 1.0  # Initial exploration rate
EPSILON_MIN = 0.01  # Minimum exploration rate
EPSILON_DECAY = 0.995  # Decay rate for exploration rate
GAMMA = 0.9
EPISODES = 200  # Number of episodes
MAX_STEPS = ROWS * COLS * 10  # Maximum number of steps

# Define maze with random barriers
barriers = set()
num_barriers = random.randint(20, 60)  # Random number of barriers
for _ in range(num_barriers):
    barrier = (random.randint(0, ROWS - 1), random.randint(0, COLS - 1))
    if barrier != START and barrier != GOAL:
        barriers.add(barrier)

# Define rewards for outside and goal states
outside_penalty = -100
goal_reward = 100

# Initialize Q-values dictionary
Q = {}

# Monte Carlo learning
start_time = time.time()
epsilon = EPSILON_START
for _ in range(EPISODES):
    state = START
    step_count = 0
    while state != GOAL and step_count < MAX_STEPS:
        if random.uniform(0, 1) < epsilon:
            action = random.randint(0, len(ACTIONS) - 1)  # Random action
        else:
            # Get action with max Q-value for the current state
            action_values = [Q.get((state, a), 0) for a in range(len(ACTIONS))]
            action = np.argmax(action_values)

        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

        # Ensure next_state is within bounds and not a barrier
        if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and next_state not in barriers:
            new_action = np.argmax([Q.get((next_state, a), 0) for a in range(len(ACTIONS))])
            Q[(state, action)] = Q.get((state, action), 0) + GAMMA * (
                    goal_reward + GAMMA * Q.get((next_state, new_action), 0) - Q.get((state, action), 0))
            state = next_state
        else:
            # Penalize the agent and reset to a random state
            state = (random.randint(0, ROWS - 1), random.randint(0, COLS - 1))

        step_count += 1

    # Decay epsilon
    epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)

# Execute learned policy
state = START
path = [state]
while state != GOAL:
    # Get action with max Q-value for the current state
    action_values = [Q.get((state, a), 0) for a in range(len(ACTIONS))]
    action = np.argmax(action_values)

    next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])

    # Ensure next_state is within bounds and not a barrier
    if 0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS and next_state not in barriers:
        path.append(next_state)
        state = next_state
    else:
        # If the next state is a barrier or outside the maze, choose a random action
        state = (random.randint(0, ROWS - 1), random.randint(0, COLS - 1))

# Print optimal path
print("Optimal Path:")
for row in range(ROWS):
    for col in range(COLS):
        if (row, col) in path:
            print("X", end=" ")
        elif (row, col) in barriers:
            print("#", end=" ")
        else:
            print(".", end=" ")
    print()

# Print execution time
print("\nExecution Time:", time.time() - start_time, "seconds")



"""Two variations of the Monte Carlo algorithm of Monte Carlo Policy on 5 * 5 maze"""

import numpy as np
import random

# Constants
ROWS = 5
COLS = 5
START_STATE = (0, 0)
GOAL_STATE = (4, 4)
ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']
EPSILON = 0.1  # Exploration rate

# Maze definition
maze = [[' ' for _ in range(COLS)] for _ in range(ROWS)]
maze[0][0] = 'S'  # Start state
maze[4][4] = 'G'  # Goal state
barriers = [(2, 1), (2, 2), (2, 3), (2, 4), (4, 2)]  # Barrier positions
for barrier in barriers:
    maze[barrier[0]][barrier[1]] = 'X'

# Define rewards for each state transition
rewards = {}
for i in range(ROWS):
    for j in range(COLS):
        for action in ACTIONS:
            if maze[i][j] == 'X':  # Check if the current state is a barrier
                rewards[(i, j), action] = -10  # High penalty for transitioning to a barrier state
            else:
                rewards[(i, j), action] = -1  # Small penalty for other actions

# Reward for reaching the goal state
rewards[GOAL_STATE, None] = 100

# Initialize equi-random policy
policy = np.ones((ROWS, COLS, len(ACTIONS))) / len(ACTIONS)

# Function to take action in the maze
def take_action(state, action):
    row, col = state
    if action == 'UP':
        row = max(0, row - 1)
    elif action == 'DOWN':
        row = min(ROWS - 1, row + 1)
    elif action == 'LEFT':
        col = max(0, col - 1)
    elif action == 'RIGHT':
        col = min(COLS - 1, col + 1)
    return (row, col)

# Function to simulate an episode using the current policy
def simulate_episode():
    episode = []
    current_state = START_STATE
    total_reward = 0
    while current_state != GOAL_STATE:
        action = np.random.choice(ACTIONS, p=policy[current_state])
        next_state = take_action(current_state, action)
        reward = rewards.get((current_state, action), 0)  # Get reward for the state transition
        total_reward += reward
        episode.append((current_state, action, reward))
        current_state = next_state
    return total_reward, episode  # Return both total_reward and episode

# First-Visit Monte Carlo Algorithm
def first_visit_monte_carlo(num_episodes=1000):
    value_function = np.zeros((ROWS, COLS))
    returns_sum = np.zeros((ROWS, COLS, len(ACTIONS)))
    returns_count = np.zeros((ROWS, COLS, len(ACTIONS)), dtype=int)

    for _ in range(num_episodes):
        episode_reward, episode = simulate_episode()
        G = 0
        visited_state_actions = set()

        for t, (state, action, reward) in enumerate(episode[::-1]):
            G += reward
            state_action = (state, action)
            if state_action not in visited_state_actions:
                visited_state_actions.add(state_action)
                action_index = ACTIONS.index(action)
                returns_sum[state][action_index] += G
                returns_count[state][action_index] += 1
                value_function[state] = np.max(returns_sum[state] / returns_count[state])

    return value_function, episode_reward

# Every-Visit Monte Carlo Algorithm
def every_visit_monte_carlo(num_episodes=1000):
    value_function = np.zeros((ROWS, COLS))
    returns_sum = np.zeros((ROWS, COLS, len(ACTIONS)))
    returns_count = np.zeros((ROWS, COLS, len(ACTIONS)), dtype=int)

    for _ in range(num_episodes):
        episode_reward, episode = simulate_episode()
        G = 0

        for t, (state, action, reward) in enumerate(episode[::-1]):
            G += reward
            action_index = ACTIONS.index(action)
            returns_sum[state][action_index] += G
            returns_count[state][action_index] += 1
            value_function[state] = np.max(returns_sum[state] / returns_count[state])

    return value_function, episode_reward

# Execute First-Visit Monte Carlo
value_function_first_visit, reward_first_visit = first_visit_monte_carlo()
print("First-Visit Monte Carlo Value Function:")
print(value_function_first_visit)
print("Reward received:", reward_first_visit)

# Execute Every-Visit Monte Carlo
value_function_every_visit, reward_every_visit = every_visit_monte_carlo()
print("\nEvery-Visit Monte Carlo Value Function:")
print(value_function_every_visit)
print("Reward received:", reward_every_visit)