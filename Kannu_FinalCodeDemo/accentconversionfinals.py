# -*- coding: utf-8 -*-
"""AccentConversionFinals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q9ddmUCxXetRMz-jUgy0ukI4AhlrC3Jc
"""

!pip install pydub

import os
import numpy as np
import librosa
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras import layers, regularizers, optimizers
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, Bidirectional,  MultiHeadAttention, Conv1D, MaxPooling1D
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
import matplotlib.pyplot as plt
from pydub import AudioSegment
from tensorflow.keras.layers.experimental import preprocessing

# Function to load data from folder
def load_data_from_folder(folder):
    labels = []
    data = []

    for subdir, _, files in os.walk(folder):
        for file in files:
            if file.endswith(".npy"):
                file_path = os.path.join(subdir, file)
                label = os.path.basename(subdir)  # Use the parent folder name as the label
                labels.append(label)
                data.append(np.load(file_path))

    return np.array(data), np.array(labels)

# Specify the root folder containing subdirectories with .npy files
root_folder = "/content/drive/MyDrive/MFCC_Features"

# Load data from subdirectories
data, labels = load_data_from_folder(root_folder)

# Encode labels
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)
num_classes = len(label_encoder.classes_)
encoded_labels = to_categorical(encoded_labels, num_classes=num_classes)

# Split the data into training and testing sets with stratification
X_train, X_test, y_train, y_test = train_test_split(
    data, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels
)

# Data normalization using TensorFlow preprocessing layers
normalization_layer = preprocessing.Normalization()
normalization_layer.adapt(X_train)  # Compute mean and variance based on training data
X_train_normalized = normalization_layer(X_train)
X_test_normalized = normalization_layer(X_test)

# Data augmentation using TensorFlow preprocessing layers
data_augmentation = Sequential([
    preprocessing.Rescaling(1./255),  # Normalize pixel values to [0,1]
    preprocessing.RandomFlip("horizontal"),
    preprocessing.RandomRotation(0.2),
])

X_train_augmented = data_augmentation(X_train_normalized)
X_test_augmented = data_augmentation(X_test_normalized)

# Compute class weights for handling class imbalance
class_weights = compute_class_weight("balanced", classes=np.unique(encoded_labels.argmax(axis=1)), y=encoded_labels.argmax(axis=1))
class_weight_dict = dict(zip(np.unique(encoded_labels.argmax(axis=1)), class_weights))

"""STARTING MFFC Feature TRAINING FROM HERE"""

# Build the LSTM model with added layers for better performance
model = Sequential()
model.add(LSTM(64, input_shape=(X_train_augmented.shape[1], X_train_augmented.shape[2])))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# Use the Adam optimizer for better convergence
optimizer = Adam(learning_rate=0.0001)

# Compile the model with categorical_crossentropy loss and accuracy metric
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model with a more expressive variable name for clarity
epochs = 10
batch_size = 22
validation_data = (X_test_augmented, y_test)

history = model.fit(X_train_augmented, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)

# Plot training history
def plot_training_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'b-', label='Training acc')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_acc, 'r-', label='Validation acc')  # Use 'r-' for solid red line
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'b-', label='Training loss')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_loss, 'r-', label='Validation loss')  # Use 'r-' for solid red line
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot the training history
plot_training_history(history)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy*100:.2f}%")

# Build the LSTM model with added bidirectional layers for better performance
model = Sequential()
model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_augmented.shape[1], X_train_augmented.shape[2])))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(64)))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# Use the Adam optimizer for better convergence
optimizer = Adam(learning_rate=0.001)

# Compile the model with categorical_crossentropy loss and accuracy metric
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model with a more expressive variable name for clarity
epochs = 10
batch_size = 32
validation_data = (X_test_augmented, y_test)

history = model.fit(X_train_augmented, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)

# Plot training history
def plot_training_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'b-', label='Training acc')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_acc, 'r-', label='Validation acc')  # Use 'r-' for solid red line
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'b-', label='Training loss')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_loss, 'r-', label='Validation loss')  # Use 'r-' for solid red line
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot the training history
plot_training_history(history)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy*100:.2f}%")

#Changing the Dropout
# Build the LSTM model with added bidirectional layers for better performance
model = Sequential()
model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train_augmented.shape[1], X_train_augmented.shape[2])))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(64)))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))

# Use the Adam optimizer for better convergence
optimizer = Adam(learning_rate=0.001)

# Compile the model with categorical_crossentropy loss and accuracy metric
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model with a more expressive variable name for clarity
epochs = 10
batch_size = 32
validation_data = (X_test_augmented, y_test)

history = model.fit(X_train_augmented, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)

# Plot training history
def plot_training_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'b-', label='Training acc')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_acc, 'r-', label='Validation acc')  # Use 'r-' for solid red line
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'b-', label='Training loss')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_loss, 'r-', label='Validation loss')  # Use 'r-' for solid red line
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot the training history
plot_training_history(history)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy*100:.2f}%")

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Bidirectional, Dense, Dropout, LSTM, Attention
from tensorflow.keras.optimizers import Adam

# Input layer
input_layer = Input(shape=(X_train_augmented.shape[1], X_train_augmented.shape[2]))

# Bidirectional LSTM with attention
lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(input_layer)
attention = Attention()([lstm_layer, lstm_layer])
dropout_1 = Dropout(0.5)(attention)

lstm_layer_2 = Bidirectional(LSTM(64))(dropout_1)
attention_2 = Attention()([lstm_layer_2, lstm_layer_2])
dropout_2 = Dropout(0.5)(attention_2)

# Dense layers
dense_1 = Dense(128, activation='relu')(dropout_2)
dropout_3 = Dropout(0.2)(dense_1)

output_layer = Dense(num_classes, activation='softmax')(dropout_3)

# Create the model
model = Model(inputs=input_layer, outputs=output_layer)

# Use the Adam optimizer for better convergence
optimizer = Adam(learning_rate=0.001)

# Compile the model with categorical_crossentropy loss and accuracy metric
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model
epochs = 10
batch_size = 32
validation_data = (X_test_augmented, y_test)

history = model.fit(X_train_augmented, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)

# Plot training history
def plot_training_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'b-', label='Training acc')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_acc, 'r-', label='Validation acc')  # Use 'r-' for solid red line
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'b-', label='Training loss')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_loss, 'r-', label='Validation loss')  # Use 'r-' for solid red line
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot the training history
plot_training_history(history)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy*100:.2f}%")

# Build the model
model = Sequential()
model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(MaxPooling1D(pool_size=2))
model.add(Bidirectional(LSTM(64, return_sequences=True)))
model.add(Bidirectional(LSTM(64)))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# Use the Adam optimizer for better convergence
model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# Train the model
epochs = 10
batch_size = 32
validation_data = (X_test, y_test)

history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)

# Evaluate the model on the test set
_, test_accuracy = model.evaluate(X_test, y_test, verbose=0)

# Print the combined accuracy result
print(f"Combined Accuracy on Test Set: {test_accuracy * 100:.2f}%")

# Plot training history
def plot_training_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'b-', label='Training acc')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_acc, 'r-', label='Validation acc')  # Use 'r-' for solid red line
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'b-', label='Training loss')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_loss, 'r-', label='Validation loss')  # Use 'r-' for solid red line
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot the training history
plot_training_history(history)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy*100:.2f}%")

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming you have a dataset X_train and y_train
# X_train should be a 2D array with shape (number_of_samples, input_size)
# y_train should be a 2D array with shape (number_of_samples, output_size)

# Define the architecture parameters
input_size = 25  # Size of input layer
hidden_size = 100  # Size of hidden layers
output_size = 25  # Size of output layer

# Generate synthetic data (replace this with your actual data loading)
X_train = np.random.randn(1000, input_size)
y_train = np.random.randn(1000, output_size)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Standardize the input data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the model
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(input_size,)),
    tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=False),
    tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=False),
    tf.keras.layers.Dense(output_size, activation=None, use_bias=False)
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Print the model summary
model.summary()

# Train the model
epochs = 10
batch_size = 32
history = model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_scaled, y_test))


# Evaluate the model on the test set
_, test_accuracy = model.evaluate(X_test, y_test, verbose=0)

# Print the combined accuracy result
print(f"Combined Accuracy on Test Set: {test_accuracy * 100:.2f}%")


# Plot training history
def plot_training_history(history):
    mae = history.history['mae']
    val_mae = history.history['val_mae']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(mae) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, mae, 'b-', label='Training MAE')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_mae, 'r-', label='Validation MAE')  # Use 'r-' for solid red line
    plt.title('Training and validation MAE')
    plt.xlabel('Epochs')
    plt.ylabel('MAE')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'b-', label='Training Loss')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_loss, 'r-', label='Validation Loss')  # Use 'r-' for solid red line
    plt.title('Training and validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot the training history
plot_training_history(history)

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming you have a dataset X_train and y_train
# X_train should be a 2D array with shape (number_of_samples, input_size)
# y_train should be a 2D array with shape (number_of_samples, output_size)

# Define the architecture parameters
input_size = 25  # Size of input layer
hidden_size = 100  # Size of hidden layers
output_size = 25  # Size of output layer

# Generate synthetic data (replace this with your actual data loading)
X_train = np.random.randn(1000, input_size)
y_train = np.random.randn(1000, output_size)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Standardize the input data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the model
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(input_size,)),
    tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=False),
    tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=False),
    tf.keras.layers.Dense(output_size, activation=None, use_bias=False)
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Print the model summary
model.summary()

# Train the model
epochs = 10
batch_size = 32
history = model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_scaled, y_test))


# Evaluate the model on the test set
_, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)

# Print the combined accuracy result
print(f"Combined Accuracy on Test Set: {test_accuracy * 100:.2f}%")


# Plot training history
def plot_training_history(history):
    mae = history.history['mae']
    val_mae = history.history['val_mae']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(mae) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, mae, 'b-', label='Training MAE')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_mae, 'r-', label='Validation MAE')  # Use 'r-' for solid red line
    plt.title('Training and validation MAE')
    plt.xlabel('Epochs')
    plt.ylabel('MAE')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'b-', label='Training Loss')  # Use 'b-' for solid blue line
    plt.plot(epochs, val_loss, 'r-', label='Validation Loss')  # Use 'r-' for solid red line
    plt.title('Training and validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot the training history
plot_training_history(history)